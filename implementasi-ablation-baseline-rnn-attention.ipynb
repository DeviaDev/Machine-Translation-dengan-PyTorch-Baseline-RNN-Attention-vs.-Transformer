{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12856749,"sourceType":"datasetVersion","datasetId":8132015},{"sourceId":12858058,"sourceType":"datasetVersion","datasetId":8132781}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls -l /kaggle/input/translate6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T02:28:38.430191Z","iopub.execute_input":"2025-08-25T02:28:38.430850Z","iopub.status.idle":"2025-08-25T02:28:38.594648Z","shell.execute_reply.started":"2025-08-25T02:28:38.430826Z","shell.execute_reply":"2025-08-25T02:28:38.593678Z"}},"outputs":[{"name":"stdout","text":"total 60\n-rw-r--r-- 1 nobody nogroup  3513 Aug 25 00:48 analisis.py\n-rw-r--r-- 1 nobody nogroup  1109 Aug 25 00:48 attention.py\n-rw-r--r-- 1 nobody nogroup  1521 Aug 25 00:48 decoder.py\n-rw-r--r-- 1 nobody nogroup   908 Aug 25 00:48 encoder.py\n-rw-r--r-- 1 nobody nogroup  9298 Aug 25 00:48 eval.py\n-rw-r--r-- 1 nobody nogroup  1738 Aug 25 00:48 heatmap.py\n-rw-r--r-- 1 nobody nogroup 11924 Aug 25 00:48 main.py\n-rw-r--r-- 1 nobody nogroup  3754 Aug 25 00:48 seq2seq.py\n-rw-r--r-- 1 nobody nogroup  2035 Aug 25 00:48 top_words.py\n-rw-r--r-- 1 nobody nogroup  5281 Aug 25 00:48 util.py\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"import unicodedata\nfrom collections import Counter\nfrom pathlib import Path\nimport argparse\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport math\nimport sacrebleu\nimport sys\nimport os\nimport csv # Tambahkan import ini untuk menyimpan riwayat CSV\n\n# Definisikan konstanta global di luar fungsi\nSPECIALS = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\nPAD, BOS, EOS, UNK = range(4)\n\n# --- Class Attention, Encoder, Decoder (sudah ada) ---\nclass BahdanauAttentionQKV(nn.Module):\n    def __init__(self, hidden_size, query_size, key_size, dropout_p=0.1):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.Wa = nn.Linear(query_size, hidden_size)\n        self.Wk = nn.Linear(key_size, hidden_size)\n        self.V = nn.Linear(hidden_size, 1)\n        self.dropout = nn.Dropout(p=dropout_p)\n\n    def forward(self, query, keys, mask=None):\n        query = self.Wa(query)\n        keys = self.Wk(keys)\n        scores = self.V(torch.tanh(query + keys))\n        scores = scores.squeeze(-1)\n        if mask is not None:\n            scores.masked_fill_(mask, -float(\"inf\"))\n        return scores\n\nclass BahdanauEncoder(nn.Module):\n    def __init__(self, input_dim, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, dropout_p=0.1):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=PAD)\n        self.rnn = nn.GRU(embedding_dim, encoder_hidden_dim, bidirectional=True)\n        self.fc = nn.Linear(encoder_hidden_dim * 2, decoder_hidden_dim)\n        self.dropout = nn.Dropout(p=dropout_p)\n\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        outputs, hidden = self.rnn(embedded)\n        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n        return outputs, hidden\n\nclass BahdanauDecoder(nn.Module):\n    def __init__(self, output_dim, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, attention, dropout_p=0.1):\n        super().__init__()\n        self.output_dim = output_dim\n        self.attention = attention\n        self.embedding = nn.Embedding(output_dim, embedding_dim, padding_idx=PAD)\n        self.rnn = nn.GRU(embedding_dim + encoder_hidden_dim * 2, decoder_hidden_dim)\n        self.fc_out = nn.Linear(embedding_dim + decoder_hidden_dim + encoder_hidden_dim * 2, output_dim)\n        self.dropout = nn.Dropout(p=dropout_p)\n        \n    def forward(self, x, hidden, encoder_outputs):\n        embedded = self.dropout(self.embedding(x))\n        encoder_outputs_T = encoder_outputs.transpose(0, 1)\n        query_for_attn = hidden.unsqueeze(1)\n        attn_weights = self.attention(query_for_attn, encoder_outputs_T)\n        attn_weights = F.softmax(attn_weights, dim=1)\n        weighted_context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs_T).squeeze(1)\n        rnn_input = torch.cat((embedded.squeeze(0), weighted_context), dim=1).unsqueeze(0)\n        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n        output = output.squeeze(0)\n        embedded = embedded.squeeze(0)\n        prediction = self.fc_out(torch.cat((output, weighted_context, embedded), dim=1))\n        return prediction.unsqueeze(0), hidden.squeeze(0), attn_weights\n\n# ---- Class Model Seq2Seq dengan Beam Search (sudah ada) ----\nclass BahdanauSeq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device, pad_id, bos_id, eos_id):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        self.pad_id = pad_id\n        self.bos_id = bos_id\n        self.eos_id = eos_id\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        trg_len, batch_size = trg.size()\n        outputs = torch.zeros(trg_len, batch_size, self.decoder.output_dim).to(self.device)\n        encoder_outputs, hidden = self.encoder(src)\n        trg_input = trg[0, :]\n\n        for t in range(1, trg_len):\n            output, hidden, _ = self.decoder(trg_input.unsqueeze(0), hidden, encoder_outputs)\n            outputs[t] = output.squeeze(0)\n            \n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(2).squeeze(0)\n            trg_input = trg[t] if teacher_force else top1\n            \n        return outputs, None\n\n    def greedy_decode(self, src, max_len=40):\n        batch_size = src.size(1)\n        encoder_outputs, hidden = self.encoder(src)\n        ys = torch.ones(1, batch_size, dtype=torch.long).fill_(self.bos_id).to(self.device)\n        \n        for _ in range(max_len - 1):\n            y_tm1 = ys[-1].unsqueeze(0)\n            output, hidden, _ = self.decoder(y_tm1, hidden, encoder_outputs)\n            \n            next_word_id = output.argmax(2)\n            \n            ys = torch.cat([ys, next_word_id], dim=0)\n\n        return ys, None\n\n    def beam_search_decode(self, src, max_len=40, beam_size=3):\n        batch_size = src.size(1)\n        encoder_outputs, hidden = self.encoder(src)\n        \n        hypotheses = torch.ones(1, batch_size, beam_size, dtype=torch.long).fill_(self.bos_id).to(self.device)\n        hyp_scores = torch.zeros(batch_size, beam_size).to(self.device)\n        \n        hidden_beams = hidden.unsqueeze(1).repeat(1, beam_size, 1)\n        hidden_beams = hidden_beams.view(batch_size * beam_size, -1)\n        \n        encoder_outputs_beams = encoder_outputs.transpose(0, 1).unsqueeze(1).repeat(1, beam_size, 1, 1).view(batch_size * beam_size, encoder_outputs.size(0), -1).transpose(0,1)\n\n        for _ in range(max_len - 1):\n            last_tokens = hypotheses[-1].view(-1, 1).transpose(0,1)\n            output, hidden_beams, _ = self.decoder(last_tokens, hidden_beams.view(batch_size * beam_size, -1), encoder_outputs_beams)\n            output = output.transpose(0, 1)\n            output = F.log_softmax(output, dim=-1)\n            \n            cand_scores = hyp_scores.unsqueeze(2) + output.view(batch_size, beam_size, -1)\n            \n            cand_scores, cand_indices = cand_scores.view(batch_size, -1).topk(beam_size, dim=-1)\n            \n            hyp_scores = cand_scores\n            \n            prev_hyp_indices = cand_indices // self.decoder.output_dim\n            new_token_indices = cand_indices % self.decoder.output_dim\n            \n            new_hypotheses = torch.zeros(hypotheses.size(0) + 1, batch_size, beam_size, dtype=torch.long).to(self.device)\n            for i in range(hypotheses.size(0)):\n                new_hypotheses[i] = torch.gather(hypotheses[i], 1, prev_hyp_indices)\n            new_hypotheses[-1] = new_token_indices\n            \n            hypotheses = new_hypotheses\n            \n            eos_mask = (new_token_indices == self.eos_id)\n            if eos_mask.all():\n                break\n\n        best_hyp_indices = hyp_scores.argmax(dim=1)\n        final_hypotheses = torch.zeros(max_len, batch_size, dtype=torch.long).to(self.device)\n\n        for b in range(batch_size):\n            best_hyp = hypotheses[:, b, best_hyp_indices[b]]\n            final_hypotheses[:len(best_hyp), b] = best_hyp\n            final_hypotheses[len(best_hyp):, b] = self.pad_id\n\n        return final_hypotheses.to(self.device)\n\n# ---- Helper functions for data processing and evaluation (sudah ada) ----\ndef normalize(text):\n    return unicodedata.normalize(\"NFKC\", text.lower().strip())\n\ndef to_ids(tokens, vocab, unk_id=3, bos_id=1, eos_id=2):\n    ids = [bos_id]\n    for tok in tokens:\n        ids.append(vocab.get(tok, unk_id))\n    ids.append(eos_id)\n    return ids\n\ndef decode_ids(ids, itos, bos_id=1, eos_id=2):\n    tokens = []\n    for i in ids:\n        if i.item() == eos_id:\n            break\n        if i.item() != bos_id:\n            tokens.append(itos[i.item()])\n    return \" \".join(tokens)\n\ndef collate_batch(batch):\n    src_list, trg_list = [], []\n    for src, trg in batch:\n        src_list.append(src)\n        trg_list.append(trg)\n    src_padded = torch.nn.utils.rnn.pad_sequence(src_list, batch_first=True, padding_value=PAD)\n    trg_padded = torch.nn.utils.rnn.pad_sequence(trg_list, batch_first=True, padding_value=PAD)\n    return src_padded, trg_padded\n\ndef load_pairs(file_path, max_len=20, max_pairs=None):\n    pairs = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for i, line in enumerate(f):\n            if max_pairs and i >= max_pairs: break\n            parts = line.strip().split('\\t')\n            src, trg = normalize(parts[0]).split(' '), normalize(parts[1]).split(' ')\n            if len(src) < max_len and len(trg) < max_len:\n                pairs.append((src, trg))\n    return pairs\n\ndef split_pairs(pairs, train_ratio=0.8, val_ratio=0.1):\n    n = len(pairs)\n    n_train = int(n * train_ratio)\n    n_val = int(n * val_ratio)\n    return pairs[:n_train], pairs[n_train:n_train + n_val], pairs[n_train + n_val:]\n\ndef build_vocab(token_lists, min_freq=1, max_size=None, specials=SPECIALS):\n    counter = Counter()\n    for toks in token_lists:\n        counter.update(toks)\n    filtered = [(w, c) for w, c in counter.items() if c >= min_freq]\n    filtered.sort(key=lambda x: (-x[1], x[0]))\n    if max_size is not None:\n        filtered = filtered[:max(0, max_size - len(specials))]\n    vocab = {sp: i for i, sp in enumerate(specials)}\n    for w, _ in filtered:\n        if w not in vocab:\n            vocab[w] = len(vocab)\n    itos = {i: w for w, i in vocab.items()}\n    return vocab, itos\n\ndef epoch_run(model, loader, criterion, optimizer, train=True, teacher_forcing=0.5):\n    model.train() if train else model.eval()\n    total_loss, total_tokens = 0.0, 0\n    device = next(model.parameters()).device\n    \n    with torch.set_grad_enabled(train):\n        for src, trg in tqdm(loader):\n            src = src.to(device).T\n            trg = trg.to(device).T\n            \n            outputs, _att = model(src, trg, teacher_forcing_ratio=teacher_forcing if train else 0.0)\n            \n            logits = outputs[1:].reshape(-1, outputs.size(-1))\n            target = trg[1:].reshape(-1)\n            \n            loss = criterion(logits, target)\n            \n            if train:\n                optimizer.zero_grad(set_to_none=True)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n            \n            n_tokens = (target != PAD).sum().item()\n            total_loss += loss.item() * n_tokens\n            total_tokens += n_tokens\n            \n    avg_loss = total_loss / max(1, total_tokens)\n    ppl = math.exp(avg_loss) if avg_loss < 20 else float(\"inf\")\n    return avg_loss, ppl\n\ndef evaluate_sacrebleu(model, loader, trg_itos=None, sp_trg=None, beam_size=1):\n    model.eval()\n    refs, hyps = [], []\n    with torch.no_grad():\n        for src, trg in tqdm(loader):\n            src, trg = src.to(model.device).T, trg.to(model.device).T\n            if beam_size > 1:\n                ys = model.beam_search_decode(src, max_len=40, beam_size=beam_size)\n            else:\n                ys, _ = model.greedy_decode(src, max_len=40)\n            \n            if sp_trg:\n                pass\n            else:\n                for y in ys.T.tolist():\n                    hyps.append(decode_ids(torch.tensor(y), trg_itos))\n            \n            if sp_trg:\n                pass\n            else:\n                for t in trg.T.tolist():\n                    refs.append(decode_ids(torch.tensor(t), trg_itos))\n    \n    refs_sacrebleu = [[ref] for ref in refs]\n    bleu = sacrebleu.corpus_bleu(hyps, refs_sacrebleu).score\n    return bleu\n\n# ---- NMTDataset class ----\nclass NMTDataset(Dataset):\n    def __init__(self, pairs, src_vocab, trg_vocab):\n        self.data = [(to_ids(src, src_vocab), to_ids(trg, trg_vocab)) for src, trg in pairs]\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx):\n        src_ids, trg_ids = self.data[idx]\n        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(trg_ids, dtype=torch.long)\n\n# Fungsi untuk menjalankan satu eksperimen penuh\ndef run_experiment(args, en_vocab, id_vocab, train_loader, val_loader, test_loader, en_itos, id_itos):\n    print(f\"\\n--- Running Experiment: Dropout={args.dropout}, Hidden Size={args.encoder_hidden_size}/{args.decoder_hidden_size} ---\")\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    encoder = BahdanauEncoder(input_dim=len(en_vocab), embedding_dim=args.encoder_embedding_dim, encoder_hidden_dim=args.encoder_hidden_size, decoder_hidden_dim=args.decoder_hidden_size, dropout_p=args.dropout)\n    attn = BahdanauAttentionQKV(hidden_size=args.decoder_hidden_size, query_size=args.decoder_hidden_size, key_size=2 * args.encoder_hidden_size, dropout_p=0.0)\n    decoder = BahdanauDecoder(output_dim=len(id_vocab), embedding_dim=args.decoder_embedding_dim, encoder_hidden_dim=args.encoder_hidden_size, decoder_hidden_dim=args.decoder_hidden_size, attention=attn, dropout_p=args.dropout)\n    \n    seq2seq = BahdanauSeq2Seq(encoder, decoder, device, pad_id=PAD, bos_id=BOS, eos_id=EOS).to(device)\n    criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n    optimizer = torch.optim.Adam(seq2seq.parameters(), lr=args.lr)\n    \n    history = {\"train_loss\": [], \"val_loss\": [], \"train_ppl\": [], \"val_ppl\": [], \"val_bleu\": []}\n    EPOCHS = args.epochs\n    best_val_bleu = -1.0 # Ubah dari val_loss ke val_bleu\n    best_epoch = 0\n    \n    for epoch in range(1, EPOCHS + 1):\n        tf = max(0.3, 0.7 - 0.04 * (epoch - 1))\n        train_loss, train_ppl = epoch_run(seq2seq, train_loader, criterion, optimizer, train=True, teacher_forcing=tf)\n        val_loss, val_ppl = epoch_run(seq2seq, val_loader, criterion, optimizer, train=False, teacher_forcing=0.0)\n        \n        val_bleu = evaluate_sacrebleu(seq2seq, val_loader, trg_itos=id_itos, beam_size=1)\n        \n        history[\"train_loss\"].append(train_loss)\n        history[\"val_loss\"].append(val_loss)\n        history[\"train_ppl\"].append(train_ppl)\n        history[\"val_ppl\"].append(val_ppl)\n        history[\"val_bleu\"].append(val_bleu)\n\n        print(f\"Epoch {epoch:02d} | TF={tf:.2f} | Train Loss {train_loss:.4f} PPL {train_ppl:.2f} | Val Loss {val_loss:.4f} PPL {val_ppl:.2f} | Val Bleu {val_bleu:.4f} \")\n\n        if val_bleu > best_val_bleu: # Ubah kriteria penyimpanan\n            best_val_bleu = val_bleu\n            best_epoch = epoch\n            torch.save(seq2seq.state_dict(), args.checkpoint)\n            print(f\"Saving best model at epoch {best_epoch} with BLEU {best_val_bleu:.4f}\")\n\n    seq2seq.load_state_dict(torch.load(args.checkpoint, map_location=device))\n    test_loss, test_ppl = epoch_run(seq2seq, test_loader, criterion, optimizer, train=False, teacher_forcing=0.0)\n    \n    test_bleu = evaluate_sacrebleu(seq2seq, test_loader, trg_itos=id_itos, beam_size=3)\n    \n    print(f\"\\n--- Experiment Results ---\")\n    print(f\"Parameters: Dropout={args.dropout}, Hidden Size={args.encoder_hidden_size}/{args.decoder_hidden_size}\")\n    print(f\"Best Val BLEU: {best_val_bleu:.4f} at Epoch {best_epoch}\")\n    print(f\"TEST | Loss {test_loss:.4f} | PPL {test_ppl:.2f} | SacreBLEU {test_bleu:.2f}\")\n    \n    # Simpan hasil ke file CSV untuk analisis\n    with open(\"ablation_study_results.csv\", \"a\", newline=\"\") as f:\n        writer = csv.writer(f)\n        if f.tell() == 0:\n            writer.writerow([\"Experiment\", \"Dropout\", \"Enc Hidden\", \"Dec Hidden\", \"Best Val BLEU\", \"Test BLEU\"])\n        writer.writerow([f\"Exp {args.exp_id}\", args.dropout, args.encoder_hidden_size, args.decoder_hidden_size, best_val_bleu, test_bleu])\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_path', type=str, default='/kaggle/input/translate5/ind-eng/ind.txt', help='Path to txt data')\n    parser.add_argument('--epochs', type=int, default=10, help='Number of training epochs')\n    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')\n    parser.add_argument('--lr', type=float, default=1e-3, help='Learning rate')\n    parser.add_argument('--max_vocab', type=int, default=None)\n    parser.add_argument('--target_lang', type=str, default='ID', help='Bahasa tujuan')\n    parser.add_argument('--checkpoint', type=str, default='bahdanau_best.pt', help='Path to save model checkpoint')\n    \n    # Tambahkan argumen untuk ablation study\n    parser.add_argument('--dropout', type=float, default=0.15)\n    parser.add_argument('--encoder_hidden_size', type=int, default=512)\n    parser.add_argument('--decoder_hidden_size', type=int, default=256)\n    parser.add_argument('--encoder_embedding_dim', type=int, default=256)\n    parser.add_argument('--decoder_embedding_dim', type=int, default=256)\n    parser.add_argument('--exp_id', type=int, default=0)\n\n    # Parsing argumen untuk data preparation\n    args, unknown = parser.parse_known_args()\n    \n    data_file = Path(args.data_path)\n    pairs = load_pairs(data_file, max_len=20, max_pairs=None)\n    train_pairs, val_pairs, test_pairs = split_pairs(pairs, 0.8, 0.1)\n    \n    en_vocab, en_itos = build_vocab([src for src, _ in train_pairs], max_size=args.max_vocab)\n    id_vocab, id_itos = build_vocab([tgt for _, tgt in train_pairs], max_size=args.max_vocab)\n    \n    train_ds = NMTDataset(train_pairs, en_vocab, id_vocab)\n    val_ds = NMTDataset(val_pairs, en_vocab, id_vocab)\n    test_ds = NMTDataset(test_pairs, en_vocab, id_vocab)\n    \n    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, collate_fn=collate_batch)\n    val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, collate_fn=collate_batch)\n    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False, collate_fn=collate_batch)\n\n    # --- Jalankan Eksperimen Ablasi ---\n    # Eksperimen 1: Baseline\n    args.exp_id = 1\n    args.dropout = 0.15\n    args.encoder_hidden_size = 512\n    args.decoder_hidden_size = 256\n    args.checkpoint = 'bahdanau_baseline.pt'\n    run_experiment(args, en_vocab, id_vocab, train_loader, val_loader, test_loader, en_itos, id_itos)\n    \n    # Eksperimen 2: Mengubah Dropout\n    args.exp_id = 2\n    args.dropout = 0.3\n    args.encoder_hidden_size = 512\n    args.decoder_hidden_size = 256\n    args.checkpoint = 'bahdanau_dropout.pt'\n    run_experiment(args, en_vocab, id_vocab, train_loader, val_loader, test_loader, en_itos, id_itos)\n    \n    # Eksperimen 3: Mengubah Hidden Size\n    args.exp_id = 3\n    args.dropout = 0.15\n    args.encoder_hidden_size = 256\n    args.decoder_hidden_size = 128\n    args.checkpoint = 'bahdanau_hidden.pt'\n    run_experiment(args, en_vocab, id_vocab, train_loader, val_loader, test_loader, en_itos, id_itos)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T02:28:38.670364Z","iopub.execute_input":"2025-08-25T02:28:38.670869Z","iopub.status.idle":"2025-08-25T02:35:23.059545Z","shell.execute_reply.started":"2025-08-25T02:28:38.670839Z","shell.execute_reply":"2025-08-25T02:35:23.058845Z"}},"outputs":[{"name":"stdout","text":"\n--- Running Experiment: Dropout=0.15, Hidden Size=512/256 ---\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 32.84it/s]\n100%|██████████| 47/47 [00:00<00:00, 78.37it/s]\n100%|██████████| 47/47 [00:01<00:00, 30.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | TF=0.70 | Train Loss 4.4536 PPL 85.94 | Val Loss 5.1197 PPL 167.29 | Val Bleu 33.4370 \nSaving best model at epoch 1 with BLEU 33.4370\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 32.84it/s]\n100%|██████████| 47/47 [00:00<00:00, 79.20it/s]\n100%|██████████| 47/47 [00:01<00:00, 31.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | TF=0.66 | Train Loss 2.2406 PPL 9.40 | Val Loss 4.9918 PPL 147.21 | Val Bleu 45.1801 \nSaving best model at epoch 2 with BLEU 45.1801\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 33.31it/s]\n100%|██████████| 47/47 [00:00<00:00, 78.87it/s]\n100%|██████████| 47/47 [00:01<00:00, 30.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | TF=0.62 | Train Loss 1.2286 PPL 3.42 | Val Loss 5.1554 PPL 173.36 | Val Bleu 37.9918 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 33.30it/s]\n100%|██████████| 47/47 [00:00<00:00, 80.26it/s]\n100%|██████████| 47/47 [00:01<00:00, 29.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | TF=0.58 | Train Loss 0.9097 PPL 2.48 | Val Loss 5.3033 PPL 201.01 | Val Bleu 25.4066 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 33.13it/s]\n100%|██████████| 47/47 [00:00<00:00, 80.11it/s]\n100%|██████████| 47/47 [00:01<00:00, 31.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | TF=0.54 | Train Loss 0.7400 PPL 2.10 | Val Loss 5.5444 PPL 255.79 | Val Bleu 63.8943 \nSaving best model at epoch 5 with BLEU 63.8943\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 32.93it/s]\n100%|██████████| 47/47 [00:00<00:00, 78.95it/s]\n100%|██████████| 47/47 [00:01<00:00, 30.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 06 | TF=0.50 | Train Loss 0.6263 PPL 1.87 | Val Loss 5.6392 PPL 281.23 | Val Bleu 34.3295 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 32.81it/s]\n100%|██████████| 47/47 [00:00<00:00, 75.86it/s]\n100%|██████████| 47/47 [00:01<00:00, 30.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | TF=0.46 | Train Loss 0.5463 PPL 1.73 | Val Loss 5.7780 PPL 323.13 | Val Bleu 39.7635 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 32.99it/s]\n100%|██████████| 47/47 [00:00<00:00, 78.42it/s]\n100%|██████████| 47/47 [00:01<00:00, 30.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | TF=0.42 | Train Loss 0.5061 PPL 1.66 | Val Loss 5.8380 PPL 343.09 | Val Bleu 45.1801 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 33.26it/s]\n100%|██████████| 47/47 [00:00<00:00, 78.51it/s]\n100%|██████████| 47/47 [00:01<00:00, 31.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 09 | TF=0.38 | Train Loss 0.4597 PPL 1.58 | Val Loss 5.9303 PPL 376.29 | Val Bleu 45.1801 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 32.68it/s]\n100%|██████████| 47/47 [00:00<00:00, 80.23it/s]\n100%|██████████| 47/47 [00:01<00:00, 30.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | TF=0.34 | Train Loss 0.4327 PPL 1.54 | Val Loss 6.0314 PPL 416.31 | Val Bleu 34.5721 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 47/47 [00:00<00:00, 67.18it/s]\n100%|██████████| 47/47 [00:01<00:00, 40.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Experiment Results ---\nParameters: Dropout=0.15, Hidden Size=512/256\nBest Val BLEU: 63.8943 at Epoch 5\nTEST | Loss 7.3285 | PPL 1523.15 | SacreBLEU 48.89\n\n--- Running Experiment: Dropout=0.3, Hidden Size=512/256 ---\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 33.28it/s]\n100%|██████████| 47/47 [00:00<00:00, 80.24it/s]\n100%|██████████| 47/47 [00:01<00:00, 30.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | TF=0.70 | Train Loss 4.5630 PPL 95.87 | Val Loss 5.0254 PPL 152.23 | Val Bleu 42.7287 \nSaving best model at epoch 1 with BLEU 42.7287\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 33.06it/s]\n100%|██████████| 47/47 [00:00<00:00, 79.15it/s]\n100%|██████████| 47/47 [00:01<00:00, 29.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | TF=0.66 | Train Loss 2.5084 PPL 12.29 | Val Loss 4.8358 PPL 125.93 | Val Bleu 34.3295 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 33.10it/s]\n100%|██████████| 47/47 [00:00<00:00, 79.35it/s]\n100%|██████████| 47/47 [00:01<00:00, 30.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | TF=0.62 | Train Loss 1.4805 PPL 4.40 | Val Loss 4.9500 PPL 141.17 | Val Bleu 56.2341 \nSaving best model at epoch 3 with BLEU 56.2341\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 32.79it/s]\n100%|██████████| 47/47 [00:00<00:00, 80.41it/s]\n100%|██████████| 47/47 [00:01<00:00, 30.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | TF=0.58 | Train Loss 1.0413 PPL 2.83 | Val Loss 5.0715 PPL 159.41 | Val Bleu 29.0715 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 33.21it/s]\n100%|██████████| 47/47 [00:00<00:00, 76.79it/s]\n100%|██████████| 47/47 [00:01<00:00, 30.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | TF=0.54 | Train Loss 0.8731 PPL 2.39 | Val Loss 5.3321 PPL 206.86 | Val Bleu 50.0000 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 32.95it/s]\n100%|██████████| 47/47 [00:00<00:00, 79.37it/s]\n100%|██████████| 47/47 [00:01<00:00, 30.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 06 | TF=0.50 | Train Loss 0.7589 PPL 2.14 | Val Loss 5.4235 PPL 226.68 | Val Bleu 39.7635 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 33.38it/s]\n100%|██████████| 47/47 [00:00<00:00, 81.41it/s]\n100%|██████████| 47/47 [00:01<00:00, 30.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | TF=0.46 | Train Loss 0.6618 PPL 1.94 | Val Loss 5.6380 PPL 280.90 | Val Bleu 35.9304 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 32.82it/s]\n100%|██████████| 47/47 [00:00<00:00, 79.73it/s]\n100%|██████████| 47/47 [00:01<00:00, 29.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | TF=0.42 | Train Loss 0.5908 PPL 1.81 | Val Loss 5.7655 PPL 319.11 | Val Bleu 29.0715 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 33.04it/s]\n100%|██████████| 47/47 [00:00<00:00, 78.35it/s]\n100%|██████████| 47/47 [00:01<00:00, 30.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 09 | TF=0.38 | Train Loss 0.5545 PPL 1.74 | Val Loss 5.7574 PPL 316.52 | Val Bleu 37.1501 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:11<00:00, 33.21it/s]\n100%|██████████| 47/47 [00:00<00:00, 80.71it/s]\n100%|██████████| 47/47 [00:01<00:00, 30.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | TF=0.34 | Train Loss 0.5383 PPL 1.71 | Val Loss 5.9943 PPL 401.12 | Val Bleu 33.4370 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 47/47 [00:00<00:00, 62.59it/s]\n100%|██████████| 47/47 [00:01<00:00, 42.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Experiment Results ---\nParameters: Dropout=0.3, Hidden Size=512/256\nBest Val BLEU: 56.2341 at Epoch 3\nTEST | Loss 6.5165 | PPL 676.19 | SacreBLEU 33.03\n\n--- Running Experiment: Dropout=0.15, Hidden Size=256/128 ---\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:10<00:00, 34.61it/s]\n100%|██████████| 47/47 [00:00<00:00, 82.47it/s]\n100%|██████████| 47/47 [00:01<00:00, 31.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | TF=0.70 | Train Loss 4.7220 PPL 112.40 | Val Loss 5.2293 PPL 186.67 | Val Bleu 42.7287 \nSaving best model at epoch 1 with BLEU 42.7287\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:10<00:00, 35.05it/s]\n100%|██████████| 47/47 [00:00<00:00, 82.70it/s]\n100%|██████████| 47/47 [00:01<00:00, 32.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | TF=0.66 | Train Loss 2.6409 PPL 14.03 | Val Loss 4.8860 PPL 132.43 | Val Bleu 45.1801 \nSaving best model at epoch 2 with BLEU 45.1801\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:10<00:00, 34.75it/s]\n100%|██████████| 47/47 [00:00<00:00, 83.09it/s]\n100%|██████████| 47/47 [00:01<00:00, 31.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | TF=0.62 | Train Loss 1.5869 PPL 4.89 | Val Loss 4.8764 PPL 131.16 | Val Bleu 33.4370 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:10<00:00, 35.04it/s]\n100%|██████████| 47/47 [00:00<00:00, 84.35it/s]\n100%|██████████| 47/47 [00:01<00:00, 32.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | TF=0.58 | Train Loss 1.0479 PPL 2.85 | Val Loss 5.0464 PPL 155.46 | Val Bleu 45.1801 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:10<00:00, 35.04it/s]\n100%|██████████| 47/47 [00:00<00:00, 82.32it/s]\n100%|██████████| 47/47 [00:01<00:00, 32.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | TF=0.54 | Train Loss 0.7856 PPL 2.19 | Val Loss 5.1161 PPL 166.69 | Val Bleu 34.3295 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:10<00:00, 35.55it/s]\n100%|██████████| 47/47 [00:00<00:00, 85.74it/s]\n100%|██████████| 47/47 [00:01<00:00, 32.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 06 | TF=0.50 | Train Loss 0.6478 PPL 1.91 | Val Loss 5.3200 PPL 204.38 | Val Bleu 33.4370 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:10<00:00, 34.75it/s]\n100%|██████████| 47/47 [00:00<00:00, 84.44it/s]\n100%|██████████| 47/47 [00:01<00:00, 31.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | TF=0.46 | Train Loss 0.5647 PPL 1.76 | Val Loss 5.4002 PPL 221.44 | Val Bleu 45.1801 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:10<00:00, 35.12it/s]\n100%|██████████| 47/47 [00:00<00:00, 83.25it/s]\n100%|██████████| 47/47 [00:01<00:00, 32.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | TF=0.42 | Train Loss 0.4964 PPL 1.64 | Val Loss 5.4534 PPL 233.55 | Val Bleu 35.9304 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:10<00:00, 34.99it/s]\n100%|██████████| 47/47 [00:00<00:00, 83.16it/s]\n100%|██████████| 47/47 [00:01<00:00, 32.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 09 | TF=0.38 | Train Loss 0.4427 PPL 1.56 | Val Loss 5.6939 PPL 297.06 | Val Bleu 34.3295 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 372/372 [00:10<00:00, 35.34it/s]\n100%|██████████| 47/47 [00:00<00:00, 82.21it/s]\n100%|██████████| 47/47 [00:01<00:00, 32.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | TF=0.34 | Train Loss 0.4076 PPL 1.50 | Val Loss 5.6355 PPL 280.20 | Val Bleu 30.7394 \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 47/47 [00:00<00:00, 70.00it/s]\n100%|██████████| 47/47 [00:01<00:00, 38.58it/s]","output_type":"stream"},{"name":"stdout","text":"\n--- Experiment Results ---\nParameters: Dropout=0.15, Hidden Size=256/128\nBest Val BLEU: 45.1801 at Epoch 2\nTEST | Loss 6.1900 | PPL 487.86 | SacreBLEU 21.36\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":61}]}