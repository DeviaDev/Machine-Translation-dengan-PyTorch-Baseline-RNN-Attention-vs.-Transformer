{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12855983,"sourceType":"datasetVersion","datasetId":8131481},{"sourceId":12858650,"sourceType":"datasetVersion","datasetId":8133124}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls -l /kaggle/input/translate7","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T13:28:02.163600Z","iopub.execute_input":"2025-08-25T13:28:02.163899Z","iopub.status.idle":"2025-08-25T13:28:02.302630Z","shell.execute_reply.started":"2025-08-25T13:28:02.163874Z","shell.execute_reply":"2025-08-25T13:28:02.301877Z"}},"outputs":[{"name":"stdout","text":"total 60\n-rw-r--r-- 1 nobody nogroup 3858 Aug 25 13:26 analisis.py\n-rw-r--r-- 1 nobody nogroup 1109 Aug 25 13:26 attention.py\n-rw-r--r-- 1 nobody nogroup 1521 Aug 25 13:26 decoder.py\n-rw-r--r-- 1 nobody nogroup  908 Aug 25 13:26 encoder.py\n-rw-r--r-- 1 nobody nogroup 4429 Aug 25 13:26 eval.py\n-rw-r--r-- 1 nobody nogroup 1692 Aug 25 13:26 heatmap.py\n-rw-r--r-- 1 nobody nogroup  862 Aug 25 13:26 prepare_data.py\n-rw-r--r-- 1 nobody nogroup 3754 Aug 25 13:26 seq2seq.py\n-rw-r--r-- 1 nobody nogroup  665 Aug 25 13:26 sp_train.py\n-rw-r--r-- 1 nobody nogroup 2335 Aug 25 13:26 top_words.py\n-rw-r--r-- 1 nobody nogroup 6537 Aug 25 13:26 transformer.py\n-rw-r--r-- 1 nobody nogroup 8006 Aug 25 13:26 util.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import unicodedata\nfrom collections import Counter\nfrom pathlib import Path\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport math\nimport sacrebleu\nimport sys\nimport os\nimport csv\nimport random\nimport numpy as np\nimport re\nfrom typing import List, Dict, Tuple, Optional\n\n# Set seeds for reproducibility\ndef set_seed(seed: int = 42):\n    \"\"\"Sets seeds for reproducibility across different devices.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# --- Global Constants ---\nSPECIALS = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\nPAD, BOS, EOS, UNK = range(4)\nCLIP = 1.0\n\n# --- Helper functions ---\ndef normalize(text: str) -> str:\n    \"\"\"Improved text normalization for consistent tokenization.\"\"\"\n    text = unicodedata.normalize(\"NFKC\", text.lower().strip())\n    text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndef to_ids(tokens: List[str], vocab: Dict[str, int]) -> List[int]:\n    \"\"\"Converts a list of tokens to a list of IDs with BOS and EOS tokens.\"\"\"\n    ids = [BOS]\n    for tok in tokens:\n        ids.append(vocab.get(tok, UNK))\n    ids.append(EOS)\n    return ids\n\ndef decode_ids(ids: List[int], itos: Dict[int, str]) -> str:\n    \"\"\"Decodes a list of IDs back to a string.\"\"\"\n    tokens = []\n    if isinstance(ids, torch.Tensor):\n        ids = ids = ids.tolist()\n    \n    for i in ids:\n        if i == EOS:\n            break\n        if i != BOS and i != PAD:\n            tokens.append(itos.get(i, '<UNK>'))\n    return ' '.join(tokens)\n\ndef collate_batch(batch: List[Tuple[torch.Tensor, torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Pads a batch of sequences and sorts them by source length.\"\"\"\n    src_list, trg_list = [], []\n    for src, trg in batch:\n        src_list.append(src)\n        trg_list.append(trg)\n    \n    sorted_batch = sorted(zip(src_list, trg_list), key=lambda x: len(x[0]), reverse=True)\n    src_list, trg_list = zip(*sorted_batch)\n    \n    src_padded = torch.nn.utils.rnn.pad_sequence(src_list, batch_first=True, padding_value=PAD)\n    trg_padded = torch.nn.utils.rnn.pad_sequence(trg_list, batch_first=True, padding_value=PAD)\n    return src_padded, trg_padded\n\ndef load_pairs(file_path: Path, max_len: int = 25, max_pairs: Optional[int] = None) -> List[Tuple[List[str], List[str]]]:\n    \"\"\"Loads and preprocesses sentence pairs from a file.\"\"\"\n    pairs = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for i, line in enumerate(f):\n            if max_pairs and i >= max_pairs:\n                break\n            parts = line.strip().split('\\t')\n            if len(parts) >= 2:\n                src, trg = normalize(parts[0]).split(' '), normalize(parts[1]).split(' ')\n                if 3 <= len(src) <= max_len and 3 <= len(trg) <= max_len:\n                    pairs.append((src, trg))\n    return pairs\n\ndef split_pairs(pairs: List[Tuple], train_ratio: float = 0.8, val_ratio: float = 0.1) -> Tuple[List, List, List]:\n    \"\"\"Shuffles and splits data into training, validation, and test sets.\"\"\"\n    random.shuffle(pairs)\n    n = len(pairs)\n    n_train = int(n * train_ratio)\n    n_val = int(n * val_ratio)\n    return pairs[:n_train], pairs[n_train:n_train + n_val], pairs[n_train + n_val:]\n\ndef build_vocab(token_lists: List[List[str]], min_freq: int = 2, max_size: int = 30000) -> Tuple[Dict, Dict]:\n    \"\"\"Builds a vocabulary with frequency and size filtering.\"\"\"\n    counter = Counter()\n    for toks in token_lists:\n        counter.update(toks)\n    \n    filtered = [w for w, c in counter.items() if c >= min_freq]\n    filtered.sort(key=lambda w: counter[w], reverse=True)\n    \n    vocab = {sp: i for i, sp in enumerate(SPECIALS)}\n    for w in filtered[:max_size - len(SPECIALS)]:\n        if w not in vocab:\n            vocab[w] = len(vocab)\n    \n    itos = {i: w for w, i in vocab.items()}\n    return vocab, itos\n\ndef evaluate_sacrebleu(model, loader: DataLoader, trg_itos: Dict[int, str], beam_size: int = 1) -> Tuple[float, float]:\n    \"\"\"Evaluates the model using SacreBLEU and chrF scores.\"\"\"\n    model.eval()\n    refs, hyps = [], []\n    \n    with torch.no_grad():\n        for src, trg in tqdm(loader, desc=\"Evaluating\"):\n            src = src.to(model.device)\n            \n            try:\n                if beam_size > 1:\n                    translated_ids = model.beam_search_decode(src, max_len=40, beam_size=beam_size)\n                else:\n                    translated_ids = model.greedy_decode(src, max_len=40)\n                    \n                for i in range(src.size(0)):\n                    hyp = decode_ids(translated_ids[i], trg_itos)\n                    ref = decode_ids(trg[i], trg_itos)\n                    \n                    if hyp.strip() and ref.strip():\n                        hyps.append(hyp)\n                        refs.append([ref])\n            \n            except Exception as e:\n                print(f\"Error in evaluation batch: {e}\")\n                continue\n    \n    if not hyps or not refs:\n        return 0.0, 0.0\n    \n    try:\n        bleu_score = sacrebleu.corpus_bleu(hyps, refs).score\n        chrf_score = sacrebleu.corpus_chrf(hyps, refs).score\n    except Exception as e:\n        print(f\"Error calculating BLEU/chrF: {e}\")\n        return 0.0, 0.0\n    \n    return bleu_score, chrf_score\n\ndef print_translations_and_analyze(model, loader: DataLoader, src_itos: Dict[int, str], trg_itos: Dict[int, str], num_examples: int = 5, beam_size: int = 3):\n    \"\"\"Prints translation examples with greedy and beam search results.\"\"\"\n    model.eval()\n    print(\"\\n--- Translation Examples & Error Analysis ---\")\n    \n    with torch.no_grad():\n        for i, (src, trg) in enumerate(loader):\n            if i >= num_examples:\n                break\n                \n            src = src.to(model.device)\n            src_seq = src[0:1]\n            trg_seq = trg[0]\n            \n            src_text = decode_ids(src_seq.squeeze(0), src_itos)\n            ref_text = decode_ids(trg_seq, trg_itos)\n            \n            print(\"--------------------------------------------------\")\n            print(f\"Source: {src_text}\")\n            print(f\"Reference: {ref_text}\")\n            \n            try:\n                ys_greedy = model.greedy_decode(src_seq, max_len=40)\n                hyp_greedy = decode_ids(ys_greedy.squeeze(0), trg_itos)\n                print(f\"Greedy: {hyp_greedy}\")\n            except Exception as e:\n                print(f\"Greedy decoding failed: {e}\")\n            \n            try:\n                ys_beam = model.beam_search_decode(src_seq, max_len=40, beam_size=beam_size)\n                if ys_beam and len(ys_beam) > 0:\n                    hyp_beam = decode_ids(ys_beam[0], trg_itos)\n                    print(f\"Beam Search (k={beam_size}): {hyp_beam}\")\n            except Exception as e:\n                print(f\"Beam search failed: {e}\")\n\nclass EarlyStopping:\n    \"\"\"Implements early stopping based on validation loss.\"\"\"\n    def __init__(self, patience: int = 10, min_delta: float = 0.001, restore_best_weights: bool = True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        self.restore_best_weights = restore_best_weights\n        self.best_weights = None\n\n    def __call__(self, val_loss: float, model: Optional[nn.Module] = None):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            if model and self.restore_best_weights:\n                self.best_weights = {k: v.clone() for k, v in model.state_dict().items()}\n        elif self.best_loss - val_loss > self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            if model and self.restore_best_weights:\n                self.best_weights = {k: v.clone() for k, v in model.state_dict().items()}\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n                if model and self.restore_best_weights and self.best_weights:\n                    model.load_state_dict(self.best_weights)\n\n# --- Dataset Class ---\nclass NMTDataset(Dataset):\n    \"\"\"Custom Dataset for Machine Translation.\"\"\"\n    def __init__(self, pairs: List[Tuple], src_vocab: Dict, trg_vocab: Dict):\n        self.src_vocab = src_vocab\n        self.trg_vocab = trg_vocab\n        self.data = []\n        \n        for src_tokens, trg_tokens in pairs:\n            src_ids = to_ids(src_tokens, src_vocab)\n            trg_ids = to_ids(trg_tokens, trg_vocab)\n            self.data.append((src_ids, trg_ids))\n\n    def __len__(self) -> int:\n        return len(self.data)\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        src_ids, trg_ids = self.data[idx]\n        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(trg_ids, dtype=torch.long)\n\n# --- Positional Encoding ---\nclass PositionalEncoding(nn.Module):\n    \"\"\"Injects positional information into embeddings.\"\"\"\n    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\n\n# --- Transformer Model (placeholder) ---\n# Replace this with your actual Transformer implementation\ntry:\n    from torch.nn import TransformerEncoderLayer, TransformerEncoder, TransformerDecoderLayer, TransformerDecoder\nexcept ImportError:\n    print(\"Warning: torch.nn.Transformer modules not found. Using dummy classes.\")\n    # Dummy classes for demonstration\n    class TransformerEncoderLayer(nn.Module):\n        def __init__(self, d_model, nhead, dim_feedforward, dropout, batch_first, norm_first):\n            super().__init__()\n            self.dummy_param = nn.Parameter(torch.randn(1))\n        def forward(self, src, src_key_padding_mask):\n            return torch.randn(src.shape)\n    \n    class TransformerEncoder(nn.Module):\n        def __init__(self, encoder_layer, num_layers):\n            super().__init__()\n            self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n        def forward(self, src, src_key_padding_mask):\n            for layer in self.layers:\n                src = layer(src, src_key_padding_mask)\n            return src\n            \n    class TransformerDecoderLayer(nn.Module):\n        def __init__(self, d_model, nhead, dim_feedforward, dropout, batch_first, norm_first):\n            super().__init__()\n            self.dummy_param = nn.Parameter(torch.randn(1))\n        def forward(self, trg, memory, tgt_mask, tgt_key_padding_mask, memory_key_padding_mask):\n            return torch.randn(trg.shape)\n\n    class TransformerDecoder(nn.Module):\n        def __init__(self, decoder_layer, num_layers):\n            super().__init__()\n            self.layers = nn.ModuleList([decoder_layer for _ in range(num_layers)])\n        def forward(self, trg, memory, tgt_mask, tgt_key_padding_mask, memory_key_padding_mask):\n            for layer in self.layers:\n                trg = layer(trg, memory, tgt_mask, tgt_key_padding_mask, memory_key_padding_mask)\n            return trg\n\nclass TransformerSeq2Seq(nn.Module):\n    \"\"\"A standard Transformer-based sequence-to-sequence model.\"\"\"\n    def __init__(self, src_vocab_size: int, trg_vocab_size: int, d_model: int = 512, nhead: int = 8,\n                 num_encoder_layers: int = 6, num_decoder_layers: int = 6, dim_feedforward: int = 2048,\n                 dropout: float = 0.1, pad_token_id: int = PAD, device: str = 'cpu'):\n        super().__init__()\n        self.d_model = d_model\n        self.device = device\n        self.pad_token_id = pad_token_id\n        \n        self.src_embedding = nn.Embedding(src_vocab_size, d_model, padding_idx=self.pad_token_id)\n        self.trg_embedding = nn.Embedding(trg_vocab_size, d_model, padding_idx=self.pad_token_id)\n        \n        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n        self.pos_decoder = PositionalEncoding(d_model, dropout=dropout)\n        \n        encoder_layers = TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout,\n            batch_first=True, norm_first=True\n        )\n        self.transformer_encoder = TransformerEncoder(encoder_layers, num_encoder_layers)\n        \n        decoder_layers = TransformerDecoderLayer(\n            d_model, nhead, dim_feedforward, dropout,\n            batch_first=True, norm_first=True\n        )\n        self.transformer_decoder = TransformerDecoder(decoder_layers, num_decoder_layers)\n        \n        self.generator = nn.Linear(d_model, trg_vocab_size)\n        self._initialize_parameters()\n    \n    def _initialize_parameters(self):\n        \"\"\"Xavier initialization for model parameters.\"\"\"\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p, gain=1.0)\n    \n    def _get_src_mask(self, src: torch.Tensor) -> torch.Tensor:\n        \"\"\"Generates a source padding mask.\"\"\"\n        return (src == self.pad_token_id)\n\n    def _get_trg_mask(self, trg: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Generates a target subsequence mask and padding mask.\"\"\"\n        trg_len = trg.size(1)\n        trg_sub_mask = torch.triu(torch.ones(trg_len, trg_len, device=self.device) * float('-inf'), diagonal=1)\n        trg_pad_mask = (trg == self.pad_token_id)\n        return trg_sub_mask, trg_pad_mask\n\n    def forward(self, src: torch.Tensor, trg: torch.Tensor) -> Tuple[torch.Tensor, None]:\n        src_mask = self._get_src_mask(src)\n        trg_sub_mask, trg_pad_mask = self._get_trg_mask(trg)\n        \n        src_emb = self.src_embedding(src) * math.sqrt(self.d_model)\n        trg_emb = self.trg_embedding(trg) * math.sqrt(self.d_model)\n        \n        src_emb = self.pos_encoder(src_emb)\n        trg_emb = self.pos_decoder(trg_emb)\n        \n        encoder_outputs = self.transformer_encoder(src_emb, src_key_padding_mask=src_mask)\n        decoder_outputs = self.transformer_decoder(\n            trg_emb, encoder_outputs,\n            tgt_mask=trg_sub_mask,\n            tgt_key_padding_mask=trg_pad_mask,\n            memory_key_padding_mask=src_mask\n        )\n        \n        outputs = self.generator(decoder_outputs)\n        return outputs, None\n\n    def greedy_decode(self, src: torch.Tensor, max_len: int = 40) -> torch.Tensor:\n        \"\"\"Performs greedy decoding to generate output sequences.\"\"\"\n        self.eval()\n        batch_size = src.size(0)\n        \n        with torch.no_grad():\n            src_mask = self._get_src_mask(src)\n            src_emb = self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model))\n            encoder_outputs = self.transformer_encoder(src_emb, src_key_padding_mask=src_mask)\n            \n            ys = torch.ones(batch_size, 1, dtype=torch.long, device=self.device).fill_(BOS)\n            \n            for _ in range(max_len - 1):\n                trg_sub_mask, trg_pad_mask = self._get_trg_mask(ys)\n                trg_emb = self.pos_decoder(self.trg_embedding(ys) * math.sqrt(self.d_model))\n                \n                decoder_outputs = self.transformer_decoder(\n                    trg_emb, encoder_outputs,\n                    tgt_mask=trg_sub_mask,\n                    tgt_key_padding_mask=trg_pad_mask,\n                    memory_key_padding_mask=src_mask\n                )\n                \n                outputs = self.generator(decoder_outputs)\n                next_word_id = outputs.argmax(dim=-1)[:, -1:]\n                ys = torch.cat([ys, next_word_id], dim=1)\n                \n                if (next_word_id == EOS).all():\n                    break\n        return ys\n\n    def beam_search_decode(self, src: torch.Tensor, max_len: int = 40, beam_size: int = 3) -> List[List[int]]:\n        \"\"\"Performs beam search decoding for a single batch example.\"\"\"\n        self.eval()\n        if src.size(0) != 1:\n            return [self.beam_search_decode(s.unsqueeze(0), max_len, beam_size)[0] for s in src]\n\n        with torch.no_grad():\n            src_mask = self._get_src_mask(src)\n            src_emb = self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model))\n            encoder_outputs = self.transformer_encoder(src_emb, src_key_padding_mask=src_mask)\n\n            beams = [([BOS], 0.0)]\n            completed_beams = []\n\n            for _ in range(max_len):\n                candidates = []\n                for seq, score in beams:\n                    if seq[-1] == EOS:\n                        completed_beams.append((seq, score / len(seq)))\n                        continue\n\n                    ys = torch.tensor([seq], dtype=torch.long, device=self.device)\n                    trg_sub_mask, trg_pad_mask = self._get_trg_mask(ys)\n                    trg_emb = self.pos_decoder(self.trg_embedding(ys) * math.sqrt(self.d_model))\n                    \n                    decoder_outputs = self.transformer_decoder(\n                        trg_emb, encoder_outputs,\n                        tgt_mask=trg_sub_mask,\n                        tgt_key_padding_mask=trg_pad_mask,\n                        memory_key_padding_mask=src_mask\n                    )\n\n                    outputs = self.generator(decoder_outputs)\n                    log_probs = F.log_softmax(outputs[0, -1], dim=-1)\n                    \n                    top_probs, top_indices = log_probs.topk(beam_size)\n                    \n                    for prob, idx in zip(top_probs, top_indices):\n                        new_seq = seq + [idx.item()]\n                        new_score = score + prob.item()\n                        candidates.append((new_seq, new_score))\n                \n                if not candidates:\n                    break\n\n                candidates.sort(key=lambda x: x[1] / len(x[0]), reverse=True)\n                beams = candidates[:beam_size]\n                \n                if not beams:\n                    break\n            \n            final_results = sorted(completed_beams + [(seq, score / len(seq)) for seq, score in beams], \n                                    key=lambda x: x[1], reverse=True)\n\n            if not final_results:\n                return [[BOS, EOS]]\n            \n            return [res[0] for res in final_results]\n\n\n# --- Loss and Scheduler ---\nclass LabelSmoothingLoss(nn.Module):\n    \"\"\"Implements label smoothing to regularize the model.\"\"\"\n    def __init__(self, smoothing: float = 0.1, ignore_index: int = PAD):\n        super().__init__()\n        self.smoothing = smoothing\n        self.ignore_index = ignore_index\n        self.criterion = nn.KLDivLoss(reduction='batchmean')\n        \n    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        pred = pred.log_softmax(dim=-1)\n        \n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (pred.size(-1) - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n            true_dist[:, self.ignore_index] = 0\n            pad_mask = (target == self.ignore_index).unsqueeze(1)\n            true_dist.masked_fill_(pad_mask, 0)\n        \n        return self.criterion(pred, true_dist)\n\nclass WarmupScheduler:\n    \"\"\"A standard Transformer learning rate scheduler.\"\"\"\n    def __init__(self, optimizer: torch.optim.Optimizer, d_model: int, warmup_steps: int = 4000, factor: float = 1.0):\n        self.optimizer = optimizer\n        self.d_model = d_model\n        self.warmup_steps = warmup_steps\n        self.factor = factor\n        self.step_num = 0\n\n    def step(self):\n        self.step_num += 1\n        lr = self.get_learning_rate()\n        for p in self.optimizer.param_groups:\n            p['lr'] = lr\n    \n    def get_learning_rate(self) -> float:\n        return self.factor * (self.d_model ** (-0.5) *\n                              min(self.step_num ** (-0.5), self.step_num * (self.warmup_steps ** (-1.5))))\n\ndef epoch_run(model, loader: DataLoader, criterion: nn.Module, optimizer: Optional[torch.optim.Optimizer],\n              train: bool = True, model_type: str = 'transformer', scheduler: Optional[WarmupScheduler] = None) -> Tuple[float, float]:\n    \"\"\"Runs a single training or validation epoch.\"\"\"\n    model.train() if train else model.eval()\n    total_loss, total_tokens = 0.0, 0\n    device = next(model.parameters()).device\n    \n    progress_bar = tqdm(loader, desc=f\"{'Training' if train else 'Validation'}\")\n    \n    with torch.set_grad_enabled(train):\n        for src, trg in progress_bar:\n            src = src.to(device)\n            trg = trg.to(device)\n            \n            if model_type == 'transformer':\n                outputs, _ = model(src, trg[:, :-1])\n                outputs_for_loss = outputs.reshape(-1, outputs.size(-1))\n                target = trg[:, 1:].contiguous().reshape(-1)\n                \n                loss = criterion(outputs_for_loss, target)\n            else:\n                raise ValueError(f\"Unsupported model type: {model_type}\")\n            \n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n                optimizer.step()\n                if scheduler:\n                    scheduler.step()\n            \n            n_tokens = (target != PAD).sum().item()\n            total_loss += loss.item() * n_tokens\n            total_tokens += n_tokens\n            \n            progress_bar.set_postfix(loss=loss.item())\n\n    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0.0\n    ppl = math.exp(avg_loss) if avg_loss < 100 else float('inf')\n    return avg_loss, ppl\n\ndef run_experiment(args, train_pairs, val_pairs, test_pairs, src_vocab, trg_vocab, src_itos, trg_itos, device, experiment_name):\n    \"\"\"Encapsulates the training and evaluation loop for an experiment.\"\"\"\n    set_seed(42)  # Reset seed for reproducibility between experiments\n    print(f\"\\n--- Starting Experiment: {experiment_name} ---\")\n    \n    train_ds = NMTDataset(train_pairs, src_vocab, trg_vocab)\n    val_ds = NMTDataset(val_pairs, src_vocab, trg_vocab)\n    test_ds = NMTDataset(test_pairs, src_vocab, trg_vocab)\n    \n    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, collate_fn=collate_batch)\n    val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, collate_fn=collate_batch)\n    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False, collate_fn=collate_batch)\n\n    model = TransformerSeq2Seq(\n        len(src_vocab), len(trg_vocab),\n        args.d_model, args.nhead,\n        args.num_enc_layers, args.num_dec_layers,\n        args.dim_feedforward, args.dropout,\n        PAD, device\n    ).to(device)\n    \n    criterion = LabelSmoothingLoss(smoothing=0.1, ignore_index=PAD)\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.98), eps=1e-9)\n    scheduler_warmup = WarmupScheduler(optimizer, args.d_model, warmup_steps=4000, factor=0.5)\n    early_stopping = EarlyStopping(patience=10, min_delta=0.001, restore_best_weights=True)\n\n    best_val_bleu = -1.0\n    checkpoint_path = f\"best_model_{experiment_name}.pt\"\n\n    for epoch in range(1, args.epochs + 1):\n        train_loss, train_ppl = epoch_run(model, train_loader, criterion, optimizer, train=True, model_type='transformer', scheduler=scheduler_warmup)\n        val_loss, val_ppl = epoch_run(model, val_loader, criterion, None, train=False, model_type='transformer')\n        \n        # Use a consistent beam size for validation to compare checkpoints fairly\n        val_bleu, val_chrf = evaluate_sacrebleu(model, val_loader, trg_itos=trg_itos, beam_size=3)\n        \n        print(f\"Epoch {epoch:02d} | Train Loss {train_loss:.4f} PPL {train_ppl:.2f} | Val Loss {val_loss:.4f} PPL {val_ppl:.2f} | Val BLEU {val_bleu:.2f} | Val chrF {val_chrf:.2f}\")\n\n        if val_bleu > best_val_bleu:\n            best_val_bleu = val_bleu\n            torch.save({\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'epoch': epoch,\n                'val_bleu': val_bleu,\n                'val_loss': val_loss,\n                'args': args\n            }, checkpoint_path)\n            print(f\"✓ Saved best model for {experiment_name} with BLEU {val_bleu:.2f}\")\n        \n        early_stopping(val_loss, model)\n        if early_stopping.early_stop:\n            print(\"Early stopping triggered!\")\n            break\n\n    # Final evaluation on the test set\n    print(f\"\\n--- Loading best model for final evaluation ({experiment_name}) ---\")\n    try:\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        print(f\"Loaded best model from epoch {checkpoint.get('epoch', 'unknown')}\")\n    except Exception as e:\n        print(f\"Could not load checkpoint: {e}\")\n        print(\"Using current model state for evaluation\")\n    \n    # Evaluate with the specified beam size for this experiment\n    test_bleu, test_chrf = evaluate_sacrebleu(model, test_loader, trg_itos=trg_itos, beam_size=args.beam_size)\n    print(f\"\\nFINAL TEST RESULTS for {experiment_name}:\")\n    print(f\"SacreBLEU: {test_bleu:.2f} | chrF: {test_chrf:.2f}\")\n\n    # Print translation examples for this experiment\n    print_translations_and_analyze(model, test_loader, src_itos, trg_itos, num_examples=5, beam_size=args.beam_size)\n    \n    return {\n        \"experiment\": experiment_name,\n        \"test_bleu\": test_bleu,\n        \"test_chrf\": test_chrf,\n        \"best_val_bleu\": best_val_bleu,\n        \"beam_size\": args.beam_size\n    }\n\ndef main():\n    parser = argparse.ArgumentParser(description='Train a Transformer-based NMT model.')\n    parser.add_argument('--data_path', type=str, default='/kaggle/input/translate3/ind-eng/ind.txt', help='Path to txt data')\n    parser.add_argument('--epochs', type=int, default=10, help='Number of training epochs')\n    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')\n    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')\n    parser.add_argument('--d_model', type=int, default=512, help='Model dimension')\n    parser.add_argument('--nhead', type=int, default=8, help='Number of attention heads')\n    parser.add_argument('--num_enc_layers', type=int, default=6, help='Number of encoder layers')\n    parser.add_argument('--num_dec_layers', type=int, default=6, help='Number of decoder layers')\n    parser.add_argument('--dim_feedforward', type=int, default=2048, help='Feedforward network dimension')\n    parser.add_argument('--dropout', type=float, default=0.1, help='Dropout rate')\n    parser.add_argument('--max_vocab', type=int, default=25000, help='Maximum vocabulary size')\n    parser.add_argument('--checkpoint', type=str, default='best_model.pt', help='Path to save model checkpoint')\n    parser.add_argument('--beam_size', type=int, default=1, help='Beam size for evaluation')\n\n    args, unknown = parser.parse_known_args()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Running on: {device}\")\n\n    # Load and preprocess data once for all experiments\n    data_file = Path(args.data_path)\n    if not data_file.exists():\n        print(f\"Error: Data file not found at {data_file}. Please check the path.\")\n        return\n    \n    print(\"Loading and preprocessing data...\")\n    pairs = load_pairs(data_file, max_len=50)\n    if not pairs:\n        print(\"Error: No valid sentence pairs found!\")\n        return\n        \n    train_pairs, val_pairs, test_pairs = split_pairs(pairs, 0.8, 0.1)\n    print(f\"Split: Train={len(train_pairs)}, Val={len(val_pairs)}, Test={len(test_pairs)}\")\n    \n    src_tokens = [src for src, _ in train_pairs]\n    trg_tokens = [trg for _, trg in train_pairs]\n    src_vocab, src_itos = build_vocab(src_tokens, min_freq=2, max_size=args.max_vocab)\n    trg_vocab, trg_itos = build_vocab(trg_tokens, min_freq=2, max_size=args.max_vocab)\n    print(f\"Source vocab size: {len(src_vocab)} | Target vocab size: {len(trg_vocab)}\")\n\n    all_results = []\n    \n    # --- Ablation Study: Beam Size ---\n    # Experiment 1: Baseline (Greedy Search)\n    args.beam_size = 1\n    results_greedy = run_experiment(args, train_pairs, val_pairs, test_pairs, src_vocab, trg_vocab, src_itos, trg_itos, device, \"GreedySearch_k1\")\n    all_results.append(results_greedy)\n\n    # Experiment 2: Beam Search\n    args.beam_size = 5\n    results_beam = run_experiment(args, train_pairs, val_pairs, test_pairs, src_vocab, trg_vocab, src_itos, trg_itos, device, \"BeamSearch_k5\")\n    all_results.append(results_beam)\n\n    # Save all results to a single CSV for easy comparison\n    results_file = \"ablation_study_results.csv\"\n    with open(results_file, \"w\", newline=\"\") as f:\n        writer = csv.DictWriter(f, fieldnames=all_results[0].keys())\n        writer.writeheader()\n        writer.writerows(all_results)\n    \n    print(\"\\n--- Ablation Study Summary ---\")\n    print(f\"Results saved to: {results_file}\")\n    \n    for res in all_results:\n        print(f\"\\nExperiment: {res['experiment']} (Beam Size: {res['beam_size']})\")\n        print(f\"Test BLEU: {res['test_bleu']:.2f}\")\n        print(f\"Test chrF: {res['test_chrf']:.2f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T13:28:02.304245Z","iopub.execute_input":"2025-08-25T13:28:02.304502Z","iopub.status.idle":"2025-08-25T16:48:27.052337Z","shell.execute_reply.started":"2025-08-25T13:28:02.304481Z","shell.execute_reply":"2025-08-25T16:48:27.051543Z"}},"outputs":[{"name":"stdout","text":"Running on: cuda\nLoading and preprocessing data...\nSplit: Train=10814, Val=1351, Test=1353\nSource vocab size: 3425 | Target vocab size: 3514\n\n--- Starting Experiment: GreedySearch_k1 ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\nTraining:   0%|          | 0/338 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\nTraining: 100%|██████████| 338/338 [00:17<00:00, 19.81it/s, loss=2.05]\nValidation: 100%|██████████| 43/43 [00:00<00:00, 64.89it/s, loss=2.68]\nEvaluating: 100%|██████████| 43/43 [09:08<00:00, 12.75s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | Train Loss 3.1578 PPL 23.52 | Val Loss 2.1828 PPL 8.87 | Val BLEU 46.71 | Val chrF 63.61\n✓ Saved best model for GreedySearch_k1 with BLEU 46.71\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.69it/s, loss=1.6] \nValidation: 100%|██████████| 43/43 [00:00<00:00, 68.58it/s, loss=2.15]\nEvaluating: 100%|██████████| 43/43 [09:10<00:00, 12.79s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | Train Loss 2.3043 PPL 10.02 | Val Loss 1.8326 PPL 6.25 | Val BLEU 85.34 | Val chrF 68.90\n✓ Saved best model for GreedySearch_k1 with BLEU 85.34\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 19.90it/s, loss=2.1] \nValidation: 100%|██████████| 43/43 [00:00<00:00, 65.42it/s, loss=1.83]\nEvaluating: 100%|██████████| 43/43 [09:23<00:00, 13.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | Train Loss 1.8970 PPL 6.67 | Val Loss 1.6123 PPL 5.01 | Val BLEU 51.93 | Val chrF 56.00\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.69it/s, loss=1.57] \nValidation: 100%|██████████| 43/43 [00:00<00:00, 68.60it/s, loss=1.54] \nEvaluating: 100%|██████████| 43/43 [09:07<00:00, 12.73s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | Train Loss 1.5958 PPL 4.93 | Val Loss 1.4162 PPL 4.12 | Val BLEU 100.00 | Val chrF 76.89\n✓ Saved best model for GreedySearch_k1 with BLEU 100.00\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.64it/s, loss=1.37] \nValidation: 100%|██████████| 43/43 [00:00<00:00, 66.70it/s, loss=1.53] \nEvaluating: 100%|██████████| 43/43 [08:56<00:00, 12.47s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | Train Loss 1.3381 PPL 3.81 | Val Loss 1.2917 PPL 3.64 | Val BLEU 74.01 | Val chrF 51.43\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.54it/s, loss=1.14] \nValidation: 100%|██████████| 43/43 [00:00<00:00, 66.03it/s, loss=1.48] \nEvaluating: 100%|██████████| 43/43 [08:48<00:00, 12.30s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 06 | Train Loss 1.1305 PPL 3.10 | Val Loss 1.2352 PPL 3.44 | Val BLEU 80.91 | Val chrF 69.69\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.77it/s, loss=1.3]  \nValidation: 100%|██████████| 43/43 [00:00<00:00, 68.37it/s, loss=1.57] \nEvaluating: 100%|██████████| 43/43 [08:46<00:00, 12.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | Train Loss 0.9705 PPL 2.64 | Val Loss 1.1708 PPL 3.22 | Val BLEU 31.24 | Val chrF 43.47\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.76it/s, loss=0.714]\nValidation: 100%|██████████| 43/43 [00:00<00:00, 67.54it/s, loss=1.45] \nEvaluating: 100%|██████████| 43/43 [08:39<00:00, 12.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | Train Loss 0.8396 PPL 2.32 | Val Loss 1.1687 PPL 3.22 | Val BLEU 86.99 | Val chrF 84.70\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.79it/s, loss=0.875]\nValidation: 100%|██████████| 43/43 [00:00<00:00, 68.64it/s, loss=1.32] \nEvaluating: 100%|██████████| 43/43 [08:19<00:00, 11.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 09 | Train Loss 0.7373 PPL 2.09 | Val Loss 1.1825 PPL 3.26 | Val BLEU 92.35 | Val chrF 82.76\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.77it/s, loss=0.773]\nValidation: 100%|██████████| 43/43 [00:00<00:00, 67.86it/s, loss=1.32] \nEvaluating: 100%|██████████| 43/43 [08:00<00:00, 11.18s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss 0.6631 PPL 1.94 | Val Loss 1.1700 PPL 3.22 | Val BLEU 73.49 | Val chrF 60.54\n\n--- Loading best model for final evaluation (GreedySearch_k1) ---\nCould not load checkpoint: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([Namespace])` or the `torch.serialization.safe_globals([Namespace])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\nUsing current model state for evaluation\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 43/43 [00:03<00:00, 11.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nFINAL TEST RESULTS for GreedySearch_k1:\nSacreBLEU: 46.05 | chrF: 48.16\n\n--- Translation Examples & Error Analysis ---\n--------------------------------------------------\nSource: <unk> or later, someone is going to have to tell tom that he needs to <unk> himself.\nReference: cepat atau <unk> seseorang harus memberi tahu tom kalau dia perlu menjaga <unk>\nGreedy: <unk> lagi pada tom perlu memberi tahu tom perlu <unk>\nBeam Search (k=1): <unk> lagi pada tom perlu memberi tahu tom perlu <unk>\n--------------------------------------------------\nSource: there are <unk> <unk> <unk> and <unk> in the <unk> <unk>\nReference: ada <unk> <unk> <unk> dan jeruk di dalam <unk> <unk>\nGreedy: ada <unk> <unk> di kota dan <unk>\nBeam Search (k=1): ada <unk> <unk> di kota dan <unk>\n--------------------------------------------------\nSource: you have a french test tomorrow, so you need to study tonight.\nReference: kamu ada ujian bahasa perancis <unk> jadi kamu harus belajar malam ini.\nGreedy: kamu punya kebiasaan itu akan belajar bahasa prancis di sekolah,\nBeam Search (k=1): kamu punya kebiasaan itu akan belajar bahasa prancis di sekolah,\n--------------------------------------------------\nSource: if you dont want to miss the train, youd better hurry.\nReference: jika anda tidak ingin ketinggalan kereta, sebaiknya anda bergegas.\nGreedy: kalau kau tidak ingin menjadi kereta, lebih baik kamu baik atau kereta,\nBeam Search (k=1): kalau kau tidak ingin menjadi kereta, lebih baik kamu baik atau kereta,\n--------------------------------------------------\nSource: if my house were a <unk> i would invite everyone i know to my birthday party.\nReference: bila rumahku <unk> aku ingin mengundang ke pesta ulang <unk> semua orang yang aku kenal.\nGreedy: kalau saja <unk> kita akan menjadi <unk> aku tahu <unk>\nBeam Search (k=1): kalau saja <unk> kita akan menjadi <unk> aku tahu <unk>\n\n--- Starting Experiment: BeamSearch_k5 ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n  warnings.warn(\nTraining: 100%|██████████| 338/338 [00:16<00:00, 20.74it/s, loss=2.05]\nValidation: 100%|██████████| 43/43 [00:00<00:00, 68.14it/s, loss=2.68]\nEvaluating: 100%|██████████| 43/43 [09:13<00:00, 12.87s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | Train Loss 3.1578 PPL 23.52 | Val Loss 2.1828 PPL 8.87 | Val BLEU 46.71 | Val chrF 63.61\n✓ Saved best model for BeamSearch_k5 with BLEU 46.71\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.85it/s, loss=1.6] \nValidation: 100%|██████████| 43/43 [00:00<00:00, 66.46it/s, loss=2.15]\nEvaluating: 100%|██████████| 43/43 [09:11<00:00, 12.84s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | Train Loss 2.3043 PPL 10.02 | Val Loss 1.8326 PPL 6.25 | Val BLEU 85.34 | Val chrF 68.90\n✓ Saved best model for BeamSearch_k5 with BLEU 85.34\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.55it/s, loss=2.1] \nValidation: 100%|██████████| 43/43 [00:00<00:00, 68.01it/s, loss=1.83]\nEvaluating: 100%|██████████| 43/43 [09:29<00:00, 13.24s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | Train Loss 1.8970 PPL 6.67 | Val Loss 1.6123 PPL 5.01 | Val BLEU 51.93 | Val chrF 56.00\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.56it/s, loss=1.57] \nValidation: 100%|██████████| 43/43 [00:00<00:00, 66.05it/s, loss=1.54] \nEvaluating: 100%|██████████| 43/43 [09:10<00:00, 12.81s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | Train Loss 1.5958 PPL 4.93 | Val Loss 1.4162 PPL 4.12 | Val BLEU 100.00 | Val chrF 76.89\n✓ Saved best model for BeamSearch_k5 with BLEU 100.00\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.37it/s, loss=1.37] \nValidation: 100%|██████████| 43/43 [00:00<00:00, 67.34it/s, loss=1.53] \nEvaluating: 100%|██████████| 43/43 [08:59<00:00, 12.56s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | Train Loss 1.3381 PPL 3.81 | Val Loss 1.2917 PPL 3.64 | Val BLEU 74.01 | Val chrF 51.43\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.55it/s, loss=1.14] \nValidation: 100%|██████████| 43/43 [00:00<00:00, 67.51it/s, loss=1.48] \nEvaluating: 100%|██████████| 43/43 [08:51<00:00, 12.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 06 | Train Loss 1.1305 PPL 3.10 | Val Loss 1.2352 PPL 3.44 | Val BLEU 80.91 | Val chrF 69.69\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.56it/s, loss=1.3]  \nValidation: 100%|██████████| 43/43 [00:00<00:00, 64.40it/s, loss=1.57] \nEvaluating: 100%|██████████| 43/43 [08:53<00:00, 12.40s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | Train Loss 0.9705 PPL 2.64 | Val Loss 1.1708 PPL 3.22 | Val BLEU 31.24 | Val chrF 43.47\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.71it/s, loss=0.714]\nValidation: 100%|██████████| 43/43 [00:00<00:00, 67.48it/s, loss=1.45] \nEvaluating: 100%|██████████| 43/43 [08:36<00:00, 12.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | Train Loss 0.8396 PPL 2.32 | Val Loss 1.1687 PPL 3.22 | Val BLEU 86.99 | Val chrF 84.70\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.81it/s, loss=0.875]\nValidation: 100%|██████████| 43/43 [00:00<00:00, 67.64it/s, loss=1.32] \nEvaluating: 100%|██████████| 43/43 [08:16<00:00, 11.55s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 09 | Train Loss 0.7373 PPL 2.09 | Val Loss 1.1825 PPL 3.26 | Val BLEU 92.35 | Val chrF 82.76\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.82it/s, loss=0.773]\nValidation: 100%|██████████| 43/43 [00:00<00:00, 68.16it/s, loss=1.32] \nEvaluating: 100%|██████████| 43/43 [08:02<00:00, 11.22s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss 0.6631 PPL 1.94 | Val Loss 1.1700 PPL 3.22 | Val BLEU 73.49 | Val chrF 60.54\n\n--- Loading best model for final evaluation (BeamSearch_k5) ---\nCould not load checkpoint: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([Namespace])` or the `torch.serialization.safe_globals([Namespace])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\nUsing current model state for evaluation\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 43/43 [17:10<00:00, 23.96s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nFINAL TEST RESULTS for BeamSearch_k5:\nSacreBLEU: 34.77 | chrF: 48.24\n\n--- Translation Examples & Error Analysis ---\n--------------------------------------------------\nSource: <unk> or later, someone is going to have to tell tom that he needs to <unk> himself.\nReference: cepat atau <unk> seseorang harus memberi tahu tom kalau dia perlu menjaga <unk>\nGreedy: <unk> lagi pada tom perlu memberi tahu tom perlu <unk>\nBeam Search (k=5): <unk> bagi tom perlu memberi tahu bahwa dia perlu <unk>\n--------------------------------------------------\nSource: there are <unk> <unk> <unk> and <unk> in the <unk> <unk>\nReference: ada <unk> <unk> <unk> dan jeruk di dalam <unk> <unk>\nGreedy: ada <unk> <unk> di kota dan <unk>\nBeam Search (k=5): ada <unk> <unk> di kota dan <unk>\n--------------------------------------------------\nSource: you have a french test tomorrow, so you need to study tonight.\nReference: kamu ada ujian bahasa perancis <unk> jadi kamu harus belajar malam ini.\nGreedy: kamu punya kebiasaan itu akan belajar bahasa prancis di sekolah,\nBeam Search (k=5): kau harusnya menghabiskan sedikit lebih baik untuk belajar bahasa <unk>\n--------------------------------------------------\nSource: if you dont want to miss the train, youd better hurry.\nReference: jika anda tidak ingin ketinggalan kereta, sebaiknya anda bergegas.\nGreedy: kalau kau tidak ingin menjadi kereta, lebih baik kamu baik atau kereta,\nBeam Search (k=5): kalau kau tidak ingin ketinggalan kereta, lebih baik kamu bergegas.\n--------------------------------------------------\nSource: if my house were a <unk> i would invite everyone i know to my birthday party.\nReference: bila rumahku <unk> aku ingin mengundang ke pesta ulang <unk> semua orang yang aku kenal.\nGreedy: kalau saja <unk> kita akan menjadi <unk> aku tahu <unk>\nBeam Search (k=5): kalau saja <unk> kita menjadi <unk> aku akan menghabiskan <unk>\n\n--- Ablation Study Summary ---\nResults saved to: ablation_study_results.csv\n\nExperiment: GreedySearch_k1 (Beam Size: 1)\nTest BLEU: 46.05\nTest chrF: 48.16\n\nExperiment: BeamSearch_k5 (Beam Size: 5)\nTest BLEU: 34.77\nTest chrF: 48.24\n","output_type":"stream"}],"execution_count":5}]}