{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12855983,"sourceType":"datasetVersion","datasetId":8131481},{"sourceId":12858650,"sourceType":"datasetVersion","datasetId":8133124}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls -l /kaggle/input/translate7","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T05:38:48.599360Z","iopub.execute_input":"2025-08-25T05:38:48.600159Z","iopub.status.idle":"2025-08-25T05:38:48.756990Z","shell.execute_reply.started":"2025-08-25T05:38:48.600130Z","shell.execute_reply":"2025-08-25T05:38:48.756216Z"}},"outputs":[{"name":"stdout","text":"total 60\n-rw-r--r-- 1 nobody nogroup 3858 Aug 25 03:13 analisis.py\n-rw-r--r-- 1 nobody nogroup 1109 Aug 25 03:13 attention.py\n-rw-r--r-- 1 nobody nogroup 1521 Aug 25 03:13 decoder.py\n-rw-r--r-- 1 nobody nogroup  908 Aug 25 03:13 encoder.py\n-rw-r--r-- 1 nobody nogroup 4429 Aug 25 03:13 eval.py\n-rw-r--r-- 1 nobody nogroup 1692 Aug 25 03:13 heatmap.py\n-rw-r--r-- 1 nobody nogroup  862 Aug 25 03:13 prepare_data.py\n-rw-r--r-- 1 nobody nogroup 3754 Aug 25 03:13 seq2seq.py\n-rw-r--r-- 1 nobody nogroup  665 Aug 25 03:13 sp_train.py\n-rw-r--r-- 1 nobody nogroup 2335 Aug 25 03:13 top_words.py\n-rw-r--r-- 1 nobody nogroup 6537 Aug 25 03:13 transformer.py\n-rw-r--r-- 1 nobody nogroup 8006 Aug 25 03:13 util.py\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import unicodedata\nfrom collections import Counter\nfrom pathlib import Path\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport math\nimport sacrebleu\nimport sys\nimport os\nimport csv\nimport random\nimport numpy as np\nimport re\nfrom typing import List, Dict, Tuple, Optional\n\n# Set seeds for reproducibility\ndef set_seed(seed: int = 42):\n    \"\"\"Sets seeds for reproducibility across different devices.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\n# --- Global Constants ---\nSPECIALS = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\nPAD, BOS, EOS, UNK = range(4)\nCLIP = 1.0\n\n# --- Improved Helper functions ---\ndef normalize(text: str) -> str:\n    \"\"\"Improved text normalization for consistent tokenization.\"\"\"\n    text = unicodedata.normalize(\"NFKC\", text.lower().strip())\n    # Remove special characters but keep basic punctuation\n    text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n    # Normalize multiple spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndef to_ids(tokens: List[str], vocab: Dict[str, int]) -> List[int]:\n    \"\"\"Converts a list of tokens to a list of IDs with BOS and EOS tokens.\"\"\"\n    ids = [BOS]\n    for tok in tokens:\n        ids.append(vocab.get(tok, UNK))\n    ids.append(EOS)\n    return ids\n\ndef decode_ids(ids: List[int], itos: Dict[int, str]) -> str:\n    \"\"\"Decodes a list of IDs back to a string.\"\"\"\n    tokens = []\n    # Ensure input is a list of integers\n    if isinstance(ids, torch.Tensor):\n        ids = ids.tolist()\n    \n    for i in ids:\n        if i == EOS:\n            break\n        if i != BOS and i != PAD:\n            tokens.append(itos.get(i, '<UNK>'))\n    \n    return ' '.join(tokens)\n\ndef collate_batch(batch: List[Tuple[torch.Tensor, torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Pads a batch of sequences and sorts them by source length.\"\"\"\n    src_list, trg_list = [], []\n    for src, trg in batch:\n        src_list.append(src)\n        trg_list.append(trg)\n    \n    # Sort by source length for more efficient packing/padding\n    sorted_batch = sorted(zip(src_list, trg_list), key=lambda x: len(x[0]), reverse=True)\n    src_list, trg_list = zip(*sorted_batch)\n    \n    src_padded = torch.nn.utils.rnn.pad_sequence(src_list, batch_first=True, padding_value=PAD)\n    trg_padded = torch.nn.utils.rnn.pad_sequence(trg_list, batch_first=True, padding_value=PAD)\n    return src_padded, trg_padded\n\ndef load_pairs(file_path: Path, max_len: int = 25, max_pairs: Optional[int] = None) -> List[Tuple[List[str], List[str]]]:\n    \"\"\"Loads and preprocesses sentence pairs from a file.\"\"\"\n    pairs = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for i, line in enumerate(f):\n            if max_pairs and i >= max_pairs:\n                break\n            parts = line.strip().split('\\t')\n            if len(parts) >= 2:\n                src, trg = normalize(parts[0]).split(' '), normalize(parts[1]).split(' ')\n                # Filter by length and ensure non-empty\n                if 3 <= len(src) <= max_len and 3 <= len(trg) <= max_len:\n                    pairs.append((src, trg))\n    return pairs\n\ndef split_pairs(pairs: List[Tuple], train_ratio: float = 0.8, val_ratio: float = 0.1) -> Tuple[List, List, List]:\n    \"\"\"Shuffles and splits data into training, validation, and test sets.\"\"\"\n    random.shuffle(pairs)\n    n = len(pairs)\n    n_train = int(n * train_ratio)\n    n_val = int(n * val_ratio)\n    return pairs[:n_train], pairs[n_train:n_train + n_val], pairs[n_train + n_val:]\n\ndef build_vocab(token_lists: List[List[str]], min_freq: int = 2, max_size: int = 30000) -> Tuple[Dict, Dict]:\n    \"\"\"Builds a vocabulary with frequency and size filtering.\"\"\"\n    counter = Counter()\n    for toks in token_lists:\n        counter.update(toks)\n    \n    # Filter by minimum frequency and size\n    filtered = [w for w, c in counter.items() if c >= min_freq]\n    filtered.sort(key=lambda w: counter[w], reverse=True)\n    \n    vocab = {sp: i for i, sp in enumerate(SPECIALS)}\n    for w in filtered[:max_size - len(SPECIALS)]:\n        if w not in vocab:\n            vocab[w] = len(vocab)\n            \n    itos = {i: w for w, i in vocab.items()}\n    return vocab, itos\n\ndef evaluate_sacrebleu(model, loader: DataLoader, trg_itos: Dict[int, str], beam_size: int = 1) -> Tuple[float, float]:\n    \"\"\"Evaluates the model using SacreBLEU and chrF scores.\"\"\"\n    model.eval()\n    refs, hyps = [], []\n    \n    with torch.no_grad():\n        for src, trg in tqdm(loader, desc=\"Evaluating\"):\n            src = src.to(model.device)\n            \n            try:\n                if beam_size > 1:\n                    translated_ids = model.beam_search_decode(src, max_len=40, beam_size=beam_size)\n                else:\n                    translated_ids = model.greedy_decode(src, max_len=40)\n                    \n                for i in range(src.size(0)):\n                    hyp = decode_ids(translated_ids[i], trg_itos)\n                    ref = decode_ids(trg[i], trg_itos)\n                    \n                    if hyp.strip() and ref.strip():\n                        hyps.append(hyp)\n                        refs.append([ref])\n            \n            except Exception as e:\n                print(f\"Error in evaluation batch: {e}\")\n                continue\n    \n    if not hyps or not refs:\n        return 0.0, 0.0\n    \n    try:\n        bleu_score = sacrebleu.corpus_bleu(hyps, refs).score\n        chrf_score = sacrebleu.corpus_chrf(hyps, refs).score\n    except Exception as e:\n        print(f\"Error calculating BLEU/chrF: {e}\")\n        return 0.0, 0.0\n    \n    return bleu_score, chrf_score\n\ndef print_translations_and_analyze(model, loader: DataLoader, src_itos: Dict[int, str], trg_itos: Dict[int, str], num_examples: int = 5):\n    \"\"\"Prints translation examples with greedy and beam search results.\"\"\"\n    model.eval()\n    print(\"\\n--- Translation Examples & Error Analysis ---\")\n    \n    with torch.no_grad():\n        for i, (src, trg) in enumerate(loader):\n            if i >= num_examples:\n                break\n                \n            src = src.to(model.device)\n            \n            # Use the first example from the batch\n            src_seq = src[0:1]\n            trg_seq = trg[0]\n            \n            src_text = decode_ids(src_seq.squeeze(0), src_itos)\n            ref_text = decode_ids(trg_seq, trg_itos)\n            \n            print(\"--------------------------------------------------\")\n            print(f\"Source: {src_text}\")\n            print(f\"Reference: {ref_text}\")\n            \n            try:\n                ys_greedy = model.greedy_decode(src_seq, max_len=40)\n                hyp_greedy = decode_ids(ys_greedy.squeeze(0), trg_itos)\n                print(f\"Greedy: {hyp_greedy}\")\n            except Exception as e:\n                print(f\"Greedy decoding failed: {e}\")\n            \n            try:\n                ys_beam = model.beam_search_decode(src_seq, max_len=40, beam_size=3)\n                if ys_beam and len(ys_beam) > 0:\n                    hyp_beam = decode_ids(ys_beam[0], trg_itos)\n                    print(f\"Beam Search (k=3): {hyp_beam}\")\n            except Exception as e:\n                print(f\"Beam search failed: {e}\")\n\nclass EarlyStopping:\n    \"\"\"Implements early stopping based on validation loss.\"\"\"\n    def __init__(self, patience: int = 10, min_delta: float = 0.001, restore_best_weights: bool = True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        self.restore_best_weights = restore_best_weights\n        self.best_weights = None\n\n    def __call__(self, val_loss: float, model: Optional[nn.Module] = None):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            if model and self.restore_best_weights:\n                self.best_weights = {k: v.clone() for k, v in model.state_dict().items()}\n        elif self.best_loss - val_loss > self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            if model and self.restore_best_weights:\n                self.best_weights = {k: v.clone() for k, v in model.state_dict().items()}\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n                if model and self.restore_best_weights and self.best_weights:\n                    model.load_state_dict(self.best_weights)\n\n# --- Dataset Class ---\nclass NMTDataset(Dataset):\n    \"\"\"Custom Dataset for Machine Translation.\"\"\"\n    def __init__(self, pairs: List[Tuple], src_vocab: Dict, trg_vocab: Dict):\n        self.src_vocab = src_vocab\n        self.trg_vocab = trg_vocab\n        self.data = []\n        \n        for src_tokens, trg_tokens in pairs:\n            src_ids = to_ids(src_tokens, src_vocab)\n            trg_ids = to_ids(trg_tokens, trg_vocab)\n            self.data.append((src_ids, trg_ids))\n\n    def __len__(self) -> int:\n        return len(self.data)\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        src_ids, trg_ids = self.data[idx]\n        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(trg_ids, dtype=torch.long)\n\n# --- Improved Positional Encoding ---\nclass PositionalEncoding(nn.Module):\n    \"\"\"Injects positional information into embeddings.\"\"\"\n    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\n\n# --- Improved Transformer Model ---\nclass TransformerSeq2Seq(nn.Module):\n    \"\"\"A standard Transformer-based sequence-to-sequence model.\"\"\"\n    def __init__(self, src_vocab_size: int, trg_vocab_size: int, d_model: int = 512, nhead: int = 8,\n                 num_encoder_layers: int = 6, num_decoder_layers: int = 6, dim_feedforward: int = 2048,\n                 dropout: float = 0.1, pad_token_id: int = PAD, device: str = 'cpu'):\n        super().__init__()\n        self.d_model = d_model\n        self.device = device\n        self.pad_token_id = pad_token_id\n        \n        self.src_embedding = nn.Embedding(src_vocab_size, d_model, padding_idx=self.pad_token_id)\n        self.trg_embedding = nn.Embedding(trg_vocab_size, d_model, padding_idx=self.pad_token_id)\n        \n        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n        self.pos_decoder = PositionalEncoding(d_model, dropout=dropout)\n        \n        encoder_layers = TransformerEncoderLayer(\n            d_model, nhead, dim_feedforward, dropout,\n            batch_first=True, norm_first=True\n        )\n        self.transformer_encoder = TransformerEncoder(encoder_layers, num_encoder_layers)\n        \n        decoder_layers = TransformerDecoderLayer(\n            d_model, nhead, dim_feedforward, dropout,\n            batch_first=True, norm_first=True\n        )\n        self.transformer_decoder = TransformerDecoder(decoder_layers, num_decoder_layers)\n        \n        self.generator = nn.Linear(d_model, trg_vocab_size)\n        self._initialize_parameters()\n    \n    def _initialize_parameters(self):\n        \"\"\"Xavier initialization for model parameters.\"\"\"\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p, gain=1.0)\n    \n    def _get_src_mask(self, src: torch.Tensor) -> torch.Tensor:\n        \"\"\"Generates a source padding mask.\"\"\"\n        return (src == self.pad_token_id)\n\n    def _get_trg_mask(self, trg: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Generates a target subsequence mask and padding mask.\"\"\"\n        trg_len = trg.size(1)\n        trg_sub_mask = torch.triu(torch.ones(trg_len, trg_len, device=self.device) * float('-inf'), diagonal=1)\n        trg_pad_mask = (trg == self.pad_token_id)\n        return trg_sub_mask, trg_pad_mask\n\n    def forward(self, src: torch.Tensor, trg: torch.Tensor) -> Tuple[torch.Tensor, None]:\n        src_mask = self._get_src_mask(src)\n        trg_sub_mask, trg_pad_mask = self._get_trg_mask(trg)\n        \n        src_emb = self.src_embedding(src) * math.sqrt(self.d_model)\n        trg_emb = self.trg_embedding(trg) * math.sqrt(self.d_model)\n        \n        src_emb = self.pos_encoder(src_emb)\n        trg_emb = self.pos_decoder(trg_emb)\n        \n        encoder_outputs = self.transformer_encoder(src_emb, src_key_padding_mask=src_mask)\n        decoder_outputs = self.transformer_decoder(\n            trg_emb, encoder_outputs,\n            tgt_mask=trg_sub_mask,\n            tgt_key_padding_mask=trg_pad_mask,\n            memory_key_padding_mask=src_mask\n        )\n        \n        outputs = self.generator(decoder_outputs)\n        return outputs, None\n\n    def greedy_decode(self, src: torch.Tensor, max_len: int = 40) -> torch.Tensor:\n        \"\"\"Performs greedy decoding to generate output sequences.\"\"\"\n        self.eval()\n        batch_size = src.size(0)\n        \n        with torch.no_grad():\n            src_mask = self._get_src_mask(src)\n            src_emb = self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model))\n            encoder_outputs = self.transformer_encoder(src_emb, src_key_padding_mask=src_mask)\n            \n            ys = torch.ones(batch_size, 1, dtype=torch.long, device=self.device).fill_(BOS)\n            \n            for _ in range(max_len - 1):\n                trg_sub_mask, trg_pad_mask = self._get_trg_mask(ys)\n                trg_emb = self.pos_decoder(self.trg_embedding(ys) * math.sqrt(self.d_model))\n                \n                decoder_outputs = self.transformer_decoder(\n                    trg_emb, encoder_outputs,\n                    tgt_mask=trg_sub_mask,\n                    tgt_key_padding_mask=trg_pad_mask,\n                    memory_key_padding_mask=src_mask\n                )\n                \n                outputs = self.generator(decoder_outputs)\n                next_word_id = outputs.argmax(dim=-1)[:, -1:]\n                ys = torch.cat([ys, next_word_id], dim=1)\n                \n                if (next_word_id == EOS).all():\n                    break\n        return ys\n\n    def beam_search_decode(self, src: torch.Tensor, max_len: int = 40, beam_size: int = 3) -> List[List[int]]:\n        \"\"\"Performs beam search decoding for a single batch example.\"\"\"\n        self.eval()\n        if src.size(0) != 1:\n            # Handle batch_size > 1 by decoding each example sequentially\n            return [self.beam_search_decode(s.unsqueeze(0), max_len, beam_size)[0] for s in src]\n\n        with torch.no_grad():\n            src_mask = self._get_src_mask(src)\n            src_emb = self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model))\n            encoder_outputs = self.transformer_encoder(src_emb, src_key_padding_mask=src_mask)\n\n            # Initialize beams with BOS token\n            beams = [([BOS], 0.0)]  # (sequence, score)\n            completed_beams = []\n\n            for _ in range(max_len):\n                candidates = []\n                for seq, score in beams:\n                    if seq[-1] == EOS:\n                        completed_beams.append((seq, score / len(seq)))\n                        continue\n\n                    ys = torch.tensor([seq], dtype=torch.long, device=self.device)\n                    trg_sub_mask, trg_pad_mask = self._get_trg_mask(ys)\n                    trg_emb = self.pos_decoder(self.trg_embedding(ys) * math.sqrt(self.d_model))\n                    \n                    decoder_outputs = self.transformer_decoder(\n                        trg_emb, encoder_outputs,\n                        tgt_mask=trg_sub_mask,\n                        tgt_key_padding_mask=trg_pad_mask,\n                        memory_key_padding_mask=src_mask\n                    )\n\n                    outputs = self.generator(decoder_outputs)\n                    log_probs = F.log_softmax(outputs[0, -1], dim=-1)\n                    \n                    top_probs, top_indices = log_probs.topk(beam_size)\n                    \n                    for prob, idx in zip(top_probs, top_indices):\n                        new_seq = seq + [idx.item()]\n                        new_score = score + prob.item()\n                        candidates.append((new_seq, new_score))\n                \n                if not candidates:\n                    break\n\n                candidates.sort(key=lambda x: x[1] / len(x[0]), reverse=True)\n                beams = candidates[:beam_size]\n                \n                if not beams:\n                    break\n            \n            final_results = sorted(completed_beams + [(seq, score / len(seq)) for seq, score in beams], \n                                    key=lambda x: x[1], reverse=True)\n\n            if not final_results:\n                return [[BOS, EOS]]\n            \n            return [res[0] for res in final_results]\n\n\n# --- Improved Loss Function ---\nclass LabelSmoothingLoss(nn.Module):\n    \"\"\"Implements label smoothing to regularize the model.\"\"\"\n    def __init__(self, smoothing: float = 0.1, ignore_index: int = PAD):\n        super().__init__()\n        self.smoothing = smoothing\n        self.ignore_index = ignore_index\n        self.criterion = nn.KLDivLoss(reduction='batchmean')\n        \n    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        pred = pred.log_softmax(dim=-1)\n        \n        with torch.no_grad():\n            # Create a uniform distribution\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (pred.size(-1) - 1))\n            \n            # Put 1.0 - smoothing at the true target index\n            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n            \n            # Mask out the PAD token contribution\n            true_dist[:, self.ignore_index] = 0\n            \n            # Handle padding in target\n            pad_mask = (target == self.ignore_index).unsqueeze(1)\n            true_dist.masked_fill_(pad_mask, 0)\n        \n        return self.criterion(pred, true_dist)\n\n\n# --- Improved Learning Rate Scheduler ---\nclass WarmupScheduler:\n    \"\"\"A standard Transformer learning rate scheduler.\"\"\"\n    def __init__(self, optimizer: torch.optim.Optimizer, d_model: int, warmup_steps: int = 4000, factor: float = 1.0):\n        self.optimizer = optimizer\n        self.d_model = d_model\n        self.warmup_steps = warmup_steps\n        self.factor = factor\n        self.step_num = 0\n\n    def step(self):\n        self.step_num += 1\n        lr = self.get_learning_rate()\n        for p in self.optimizer.param_groups:\n            p['lr'] = lr\n    \n    def get_learning_rate(self) -> float:\n        return self.factor * (self.d_model ** (-0.5) *\n                              min(self.step_num ** (-0.5), self.step_num * (self.warmup_steps ** (-1.5))))\n\ndef epoch_run(model, loader: DataLoader, criterion: nn.Module, optimizer: Optional[torch.optim.Optimizer],\n              train: bool = True, model_type: str = 'transformer', scheduler: Optional[WarmupScheduler] = None) -> Tuple[float, float]:\n    \"\"\"Runs a single training or validation epoch.\"\"\"\n    model.train() if train else model.eval()\n    total_loss, total_tokens = 0.0, 0\n    device = next(model.parameters()).device\n    \n    progress_bar = tqdm(loader, desc=f\"{'Training' if train else 'Validation'}\")\n    \n    with torch.set_grad_enabled(train):\n        for src, trg in progress_bar:\n            src = src.to(device)\n            trg = trg.to(device)\n            \n            if model_type == 'transformer':\n                # Shift target for training\n                outputs, _ = model(src, trg[:, :-1])\n                # Reshape for loss calculation\n                outputs_for_loss = outputs.reshape(-1, outputs.size(-1))\n                target = trg[:, 1:].contiguous().reshape(-1)\n                \n                loss = criterion(outputs_for_loss, target)\n            else:\n                raise ValueError(f\"Unsupported model type: {model_type}\")\n            \n            if train:\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n                optimizer.step()\n                if scheduler:\n                    scheduler.step()\n            \n            n_tokens = (target != PAD).sum().item()\n            total_loss += loss.item() * n_tokens\n            total_tokens += n_tokens\n            \n            progress_bar.set_postfix(loss=loss.item())\n\n    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0.0\n    ppl = math.exp(avg_loss) if avg_loss < 100 else float('inf')\n    return avg_loss, ppl\n\ndef main():\n    parser = argparse.ArgumentParser(description='Train a Transformer-based NMT model.')\n    parser.add_argument('--data_path', type=str, default='/kaggle/input/translate3/ind-eng/ind.txt', help='Path to txt data')\n    parser.add_argument('--epochs', type=int, default=10, help='Number of training epochs')\n    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')\n    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')\n    parser.add_argument('--d_model', type=int, default=512, help='Model dimension')\n    parser.add_argument('--nhead', type=int, default=8, help='Number of attention heads')\n    parser.add_argument('--num_enc_layers', type=int, default=6, help='Number of encoder layers')\n    parser.add_argument('--num_dec_layers', type=int, default=6, help='Number of decoder layers')\n    parser.add_argument('--dim_feedforward', type=int, default=2048, help='Feedforward network dimension')\n    parser.add_argument('--dropout', type=float, default=0.1, help='Dropout rate')\n    parser.add_argument('--max_vocab', type=int, default=25000, help='Maximum vocabulary size')\n    parser.add_argument('--checkpoint', type=str, default='best_model.pt', help='Path to save model checkpoint')\n    parser.add_argument('--beam_size', type=int, default=1, help='Beam size for evaluation')\n    \n    args, unknown = parser.parse_known_args()\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Running on: {device}\")\n\n    # Load and preprocess data\n    data_file = Path(args.data_path)\n    if not data_file.exists():\n        print(f\"Error: Data file not found at {data_file}. Please check the path.\")\n        return\n        \n    print(\"Loading and preprocessing data...\")\n    pairs = load_pairs(data_file, max_len=50)\n    print(f\"Loaded {len(pairs)} sentence pairs\")\n    \n    if len(pairs) == 0:\n        print(\"Error: No valid sentence pairs found!\")\n        return\n    \n    train_pairs, val_pairs, test_pairs = split_pairs(pairs, 0.8, 0.1)\n    print(f\"Split: Train={len(train_pairs)}, Val={len(val_pairs)}, Test={len(test_pairs)}\")\n\n    # Build vocabularies\n    print(\"Building vocabularies...\")\n    src_tokens = [src for src, _ in train_pairs]\n    trg_tokens = [trg for _, trg in train_pairs]\n    \n    src_vocab, src_itos = build_vocab(src_tokens, min_freq=2, max_size=args.max_vocab)\n    trg_vocab, trg_itos = build_vocab(trg_tokens, min_freq=2, max_size=args.max_vocab)\n\n    print(f\"Source vocab size: {len(src_vocab)} | Target vocab size: {len(trg_vocab)}\")\n\n    # Create datasets\n    train_ds = NMTDataset(train_pairs, src_vocab, trg_vocab)\n    val_ds = NMTDataset(val_pairs, src_vocab, trg_vocab)\n    test_ds = NMTDataset(test_pairs, src_vocab, trg_vocab)\n    \n    print(f\"Dataset sizes: Train={len(train_ds)}, Val={len(val_ds)}, Test={len(test_ds)}\")\n\n    # Create data loaders\n    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, collate_fn=collate_batch)\n    val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, collate_fn=collate_batch)\n    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False, collate_fn=collate_batch)\n\n    # Create model\n    print(\"Creating Transformer model...\")\n    model = TransformerSeq2Seq(\n        len(src_vocab), len(trg_vocab),\n        args.d_model, args.nhead,\n        args.num_enc_layers, args.num_dec_layers,\n        args.dim_feedforward, args.dropout,\n        PAD, device\n    ).to(device)\n    \n    # Loss and optimizer\n    criterion = LabelSmoothingLoss(smoothing=0.1, ignore_index=PAD)\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.98), eps=1e-9)\n    \n    # Learning rate scheduler\n    scheduler_warmup = WarmupScheduler(optimizer, args.d_model, warmup_steps=4000, factor=0.5)\n    \n    # Early stopping\n    early_stopping = EarlyStopping(patience=10, min_delta=0.001, restore_best_weights=True)\n\n    # Training history\n    best_val_bleu = -1.0\n    history = {\"train_loss\": [], \"val_loss\": [], \"train_ppl\": [], \"val_ppl\": [], \"val_bleu\": [], \"val_chrf\": []}\n\n    print(f\"Starting training for {args.epochs} epochs...\")\n    \n    for epoch in range(1, args.epochs + 1):\n        # Training\n        train_loss, train_ppl = epoch_run(\n            model, train_loader, criterion, optimizer,\n            train=True, model_type='transformer', scheduler=scheduler_warmup\n        )\n        \n        # Validation\n        val_loss, val_ppl = epoch_run(\n            model, val_loader, criterion, None,\n            train=False, model_type='transformer'\n        )\n        \n        # Evaluation\n        val_bleu, val_chrf = evaluate_sacrebleu(model, val_loader, trg_itos=trg_itos, beam_size=args.beam_size)\n        \n        # Update history\n        history[\"train_loss\"].append(train_loss)\n        history[\"val_loss\"].append(val_loss)\n        history[\"train_ppl\"].append(train_ppl)\n        history[\"val_ppl\"].append(val_ppl)\n        history[\"val_bleu\"].append(val_bleu)\n        history[\"val_chrf\"].append(val_chrf)\n\n        print(f\"Epoch {epoch:02d} | Train Loss {train_loss:.4f} PPL {train_ppl:.2f} | \"\n              f\"Val Loss {val_loss:.4f} PPL {val_ppl:.2f} | \"\n              f\"Val BLEU {val_bleu:.2f} | Val chrF {val_chrf:.2f}\")\n\n        # Save best model\n        if val_bleu > best_val_bleu:\n            best_val_bleu = val_bleu\n            torch.save({\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'epoch': epoch,\n                'val_bleu': val_bleu,\n                'val_loss': val_loss,\n                'src_vocab': src_vocab,\n                'trg_vocab': trg_vocab,\n                'src_itos': src_itos,\n                'trg_itos': trg_itos,\n                'args': args\n            }, args.checkpoint)\n            print(f\"✓ Saved best model with BLEU {val_bleu:.2f}\")\n        \n        # Early stopping\n        early_stopping(val_loss, model)\n        if early_stopping.early_stop:\n            print(\"Early stopping triggered!\")\n            break\n\n    # Final evaluation\n    print(\"\\n--- Loading best model for final evaluation ---\")\n    try:\n        checkpoint = torch.load(args.checkpoint, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        print(f\"Loaded best model from epoch {checkpoint.get('epoch', 'unknown')}\")\n    except Exception as e:\n        print(f\"Could not load checkpoint: {e}\")\n        print(\"Using current model state for evaluation\")\n    \n    test_loss, test_ppl = epoch_run(model, test_loader, criterion, None, train=False, model_type='transformer')\n    test_bleu, test_chrf = evaluate_sacrebleu(model, test_loader, trg_itos=trg_itos, beam_size=args.beam_size)\n    \n    # Save results\n    results_file = f\"final_results_transformer.csv\"\n    with open(results_file, \"w\", newline=\"\") as f:\n        writer = csv.writer(f)\n        writer.writerow([\"Model\", \"Test_Loss\", \"Test_PPL\", \"Test_BLEU\", \"Test_chrF\", \"Best_Val_BLEU\"])\n        writer.writerow([\"transformer\", test_loss, test_ppl, test_bleu, test_chrf, best_val_bleu])\n    \n    print(f\"\\nFINAL TEST RESULTS:\")\n    print(f\"Loss: {test_loss:.4f} | PPL: {test_ppl:.2f}\")\n    print(f\"SacreBLEU: {test_bleu:.2f} | chrF: {test_chrf:.2f}\")\n    print(f\"Best Val BLEU: {best_val_bleu:.2f}\")\n    print(f\"Results saved to: {results_file}\")\n    \n    # Show translation examples\n    print_translations_and_analyze(model, test_loader, src_itos, trg_itos, num_examples=10)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T05:38:48.758774Z","iopub.execute_input":"2025-08-25T05:38:48.759031Z","iopub.status.idle":"2025-08-25T05:43:13.657550Z","shell.execute_reply.started":"2025-08-25T05:38:48.759009Z","shell.execute_reply":"2025-08-25T05:43:13.656679Z"}},"outputs":[{"name":"stdout","text":"Running on: cuda\nLoading and preprocessing data...\nLoaded 13518 sentence pairs\nSplit: Train=10814, Val=1351, Test=1353\nBuilding vocabularies...\nSource vocab size: 3457 | Target vocab size: 3530\nDataset sizes: Train=10814, Val=1351, Test=1353\nCreating Transformer model...\nStarting training for 10 epochs...\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.49it/s, loss=2.88]\nValidation: 100%|██████████| 43/43 [00:00<00:00, 65.68it/s, loss=2.43]\nEvaluating: 100%|██████████| 43/43 [00:08<00:00,  4.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | Train Loss 3.2186 PPL 24.99 | Val Loss 2.2017 PPL 9.04 | Val BLEU 48.89 | Val chrF 56.65\n✓ Saved best model with BLEU 48.89\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.34it/s, loss=1.97] \nValidation: 100%|██████████| 43/43 [00:00<00:00, 65.15it/s, loss=2.09]\nEvaluating: 100%|██████████| 43/43 [00:06<00:00,  7.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | Train Loss 2.2449 PPL 9.44 | Val Loss 1.8421 PPL 6.31 | Val BLEU 2.82 | Val chrF 15.87\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.53it/s, loss=1.99] \nValidation: 100%|██████████| 43/43 [00:00<00:00, 64.79it/s, loss=1.7] \nEvaluating: 100%|██████████| 43/43 [00:07<00:00,  5.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | Train Loss 1.8868 PPL 6.60 | Val Loss 1.5945 PPL 4.93 | Val BLEU 3.07 | Val chrF 18.04\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.51it/s, loss=1.55] \nValidation: 100%|██████████| 43/43 [00:00<00:00, 64.61it/s, loss=1.62]\nEvaluating: 100%|██████████| 43/43 [00:08<00:00,  5.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | Train Loss 1.5938 PPL 4.92 | Val Loss 1.4207 PPL 4.14 | Val BLEU 24.45 | Val chrF 47.40\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.56it/s, loss=0.698]\nValidation: 100%|██████████| 43/43 [00:00<00:00, 65.77it/s, loss=1.29] \nEvaluating: 100%|██████████| 43/43 [00:07<00:00,  5.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | Train Loss 1.3396 PPL 3.82 | Val Loss 1.3427 PPL 3.83 | Val BLEU 56.55 | Val chrF 46.05\n✓ Saved best model with BLEU 56.55\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.26it/s, loss=1.44] \nValidation: 100%|██████████| 43/43 [00:00<00:00, 65.93it/s, loss=1.23] \nEvaluating: 100%|██████████| 43/43 [00:04<00:00,  8.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 06 | Train Loss 1.1333 PPL 3.11 | Val Loss 1.2549 PPL 3.51 | Val BLEU 37.99 | Val chrF 73.04\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.50it/s, loss=1.22] \nValidation: 100%|██████████| 43/43 [00:00<00:00, 65.76it/s, loss=1.18] \nEvaluating: 100%|██████████| 43/43 [00:08<00:00,  5.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | Train Loss 0.9668 PPL 2.63 | Val Loss 1.2058 PPL 3.34 | Val BLEU 29.50 | Val chrF 48.90\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.52it/s, loss=0.587]\nValidation: 100%|██████████| 43/43 [00:00<00:00, 65.51it/s, loss=1.09] \nEvaluating: 100%|██████████| 43/43 [00:08<00:00,  5.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | Train Loss 0.8420 PPL 2.32 | Val Loss 1.1583 PPL 3.18 | Val BLEU 64.59 | Val chrF 65.33\n✓ Saved best model with BLEU 64.59\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.41it/s, loss=0.861]\nValidation: 100%|██████████| 43/43 [00:00<00:00, 65.07it/s, loss=1.1]  \nEvaluating: 100%|██████████| 43/43 [00:07<00:00,  5.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 09 | Train Loss 0.7434 PPL 2.10 | Val Loss 1.1630 PPL 3.20 | Val BLEU 87.62 | Val chrF 69.71\n✓ Saved best model with BLEU 87.62\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 338/338 [00:16<00:00, 20.47it/s, loss=0.915]\nValidation: 100%|██████████| 43/43 [00:00<00:00, 65.00it/s, loss=1.21] \nEvaluating: 100%|██████████| 43/43 [00:05<00:00,  7.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss 0.6649 PPL 1.94 | Val Loss 1.1610 PPL 3.19 | Val BLEU 33.26 | Val chrF 55.58\n\n--- Loading best model for final evaluation ---\nCould not load checkpoint: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([Namespace])` or the `torch.serialization.safe_globals([Namespace])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\nUsing current model state for evaluation\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 43/43 [00:00<00:00, 65.71it/s, loss=1.29] \nEvaluating: 100%|██████████| 43/43 [00:04<00:00,  8.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nFINAL TEST RESULTS:\nLoss: 1.1928 | PPL: 3.30\nSacreBLEU: 52.43 | chrF: 49.03\nBest Val BLEU: 87.62\nResults saved to: final_results_transformer.csv\n\n--- Translation Examples & Error Analysis ---\n--------------------------------------------------\nSource: if you turn here, you can probably avoid a lot of traffic.\nReference: kalau kamu belok ke sini, mungkin kamu bisa terhindar dari kemacetan.\nGreedy: kalau kamu <unk> bisa <unk> mungkin dia masih melakukan ini?\nBeam Search (k=3): kalau kau bisa <unk> di luar biasa.\n--------------------------------------------------\nSource: she was in the hospital for six weeks because of her <unk>\nReference: dia berada di rumah sakit selama enam minggu karena sakitnya\nGreedy: dia berada di ruangan itu karena enam belas tahun.\nBeam Search (k=3): dia berada di ruangan itu karena enam belas tahun.\n--------------------------------------------------\nSource: we learned about the <unk> of eating a healthy lunch.\nReference: kami mempelajari tentang <unk> memakan makan siang yang sehat.\nGreedy: kita tidak sengaja mendengar tentang makan apa yang terjadi makan siang itu makan siang.\nBeam Search (k=3): kita tidak melihat <unk> makan <unk> itu makan siang.\n--------------------------------------------------\nSource: its been a long time since ive heard anyone use that word.\nReference: sudah lama aku tidak mendengar seseorang menggunakan kata itu.\nGreedy: sudah lama sejak aku telah terjadi sejak aku makan itu dengan hal itu.\nBeam Search (k=3): sudah lama sejak pukul enam puluh menit.\n--------------------------------------------------\nSource: if i had been <unk> i would have given you some money.\nReference: jika saya <unk> saya akan memberimu uang.\nGreedy: kalau saja saya <unk> saya <unk>\nBeam Search (k=3): kalau saja aku lebih kecil daripada apa saja kau punya uang.\n--------------------------------------------------\nSource: if you say exactly whats on your mind all the time, youre likely to <unk> a lot of people.\nReference: kalau setiap waktu kamu langsung mengatakan apa yang ada dalam <unk> kamu akan membuat banyak orang merasa <unk>\nGreedy: kalau kau memberi tahu apa yang telah terjadi di dalam <unk>\nBeam Search (k=3): jika kau memberi tahu apa yang telah terjadi pada hari ulang katakan.\n--------------------------------------------------\nSource: the <unk> are <unk> at the <unk> of <unk> of thousands of <unk> a day.\nReference: <unk> hujan semakin <unk> dengan <unk> sepuluh ribu <unk> per hari.\nGreedy: <unk> itu <unk> pada <unk> <unk> <unk>\nBeam Search (k=3): <unk> itu <unk> pada <unk>\n--------------------------------------------------\nSource: this room is very small, so it is <unk> to put more <unk> in it.\nReference: ruangan ini terlalu sempit. <unk> untuk <unk> <unk> lagi di sini.\nGreedy: ruangan ini sangat bagus.\nBeam Search (k=3): ruangan ini sangat bagus.\n--------------------------------------------------\nSource: ive been in that <unk> of work for five years.\nReference: aku sudah melakukan pekerjaan itu selama lima tahun.\nGreedy: saya sudah berada di ruangan itu selama enam tahun selama enam tahun yang telah terjadi setiap tahun.\nBeam Search (k=3): saya sudah berada di ruangan itu selama 30 tahun.\n--------------------------------------------------\nSource: mt. everest is the tallest mountain in the world.\nReference: gunung everest adalah gunung tertinggi di dunia.\nGreedy: gunung tertinggi di dunia.\nBeam Search (k=3): gunung everest adalah gunung tertinggi di dunia.\n","output_type":"stream"}],"execution_count":25}]}